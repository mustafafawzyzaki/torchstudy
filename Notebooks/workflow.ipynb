{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Segment</th>\n",
       "      <th>Country</th>\n",
       "      <th>Product</th>\n",
       "      <th>Discount Band</th>\n",
       "      <th>Units Sold</th>\n",
       "      <th>Manufacturing Price</th>\n",
       "      <th>Sale Price</th>\n",
       "      <th>Gross Sales</th>\n",
       "      <th>Discounts</th>\n",
       "      <th>Sales</th>\n",
       "      <th>COGS</th>\n",
       "      <th>Profit</th>\n",
       "      <th>Date</th>\n",
       "      <th>Month Number</th>\n",
       "      <th>Month Name</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Government</td>\n",
       "      <td>Germany</td>\n",
       "      <td>-1.045228</td>\n",
       "      <td>1.874616</td>\n",
       "      <td>1513.0</td>\n",
       "      <td>-0.861342</td>\n",
       "      <td>1.694287</td>\n",
       "      <td>1.364884</td>\n",
       "      <td>-0.573087</td>\n",
       "      <td>529550.00</td>\n",
       "      <td>1.216891</td>\n",
       "      <td>136170.00</td>\n",
       "      <td>2014-12-01</td>\n",
       "      <td>12</td>\n",
       "      <td>December</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Government</td>\n",
       "      <td>Germany</td>\n",
       "      <td>0.209763</td>\n",
       "      <td>1.874616</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>-0.796841</td>\n",
       "      <td>1.694287</td>\n",
       "      <td>0.666484</td>\n",
       "      <td>-0.573087</td>\n",
       "      <td>352100.00</td>\n",
       "      <td>0.569826</td>\n",
       "      <td>90540.00</td>\n",
       "      <td>2014-06-01</td>\n",
       "      <td>6</td>\n",
       "      <td>June</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Government</td>\n",
       "      <td>Canada</td>\n",
       "      <td>0.209763</td>\n",
       "      <td>1.874616</td>\n",
       "      <td>1725.0</td>\n",
       "      <td>-0.796841</td>\n",
       "      <td>1.694287</td>\n",
       "      <td>1.656917</td>\n",
       "      <td>-0.573087</td>\n",
       "      <td>603750.00</td>\n",
       "      <td>1.487458</td>\n",
       "      <td>155250.00</td>\n",
       "      <td>2013-11-01</td>\n",
       "      <td>11</td>\n",
       "      <td>November</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Government</td>\n",
       "      <td>Germany</td>\n",
       "      <td>0.209763</td>\n",
       "      <td>1.874616</td>\n",
       "      <td>1513.0</td>\n",
       "      <td>-0.796841</td>\n",
       "      <td>1.694287</td>\n",
       "      <td>1.364884</td>\n",
       "      <td>-0.573087</td>\n",
       "      <td>529550.00</td>\n",
       "      <td>1.216891</td>\n",
       "      <td>136170.00</td>\n",
       "      <td>2014-12-01</td>\n",
       "      <td>12</td>\n",
       "      <td>December</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Government</td>\n",
       "      <td>Germany</td>\n",
       "      <td>1.464753</td>\n",
       "      <td>1.874616</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>0.216751</td>\n",
       "      <td>1.694287</td>\n",
       "      <td>0.666484</td>\n",
       "      <td>-0.573087</td>\n",
       "      <td>352100.00</td>\n",
       "      <td>0.569826</td>\n",
       "      <td>90540.00</td>\n",
       "      <td>2014-06-01</td>\n",
       "      <td>6</td>\n",
       "      <td>June</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>Midmarket</td>\n",
       "      <td>Canada</td>\n",
       "      <td>0.209763</td>\n",
       "      <td>-1.160614</td>\n",
       "      <td>1614.0</td>\n",
       "      <td>-0.796841</td>\n",
       "      <td>-0.756733</td>\n",
       "      <td>-0.624012</td>\n",
       "      <td>-0.414828</td>\n",
       "      <td>20578.50</td>\n",
       "      <td>-0.634868</td>\n",
       "      <td>4438.50</td>\n",
       "      <td>2014-04-01</td>\n",
       "      <td>4</td>\n",
       "      <td>April</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>Midmarket</td>\n",
       "      <td>Canada</td>\n",
       "      <td>0.209763</td>\n",
       "      <td>-1.160614</td>\n",
       "      <td>2559.0</td>\n",
       "      <td>-0.796841</td>\n",
       "      <td>-0.756733</td>\n",
       "      <td>-0.568223</td>\n",
       "      <td>-0.322167</td>\n",
       "      <td>32627.25</td>\n",
       "      <td>-0.588481</td>\n",
       "      <td>7037.25</td>\n",
       "      <td>2014-08-01</td>\n",
       "      <td>8</td>\n",
       "      <td>August</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>Enterprise</td>\n",
       "      <td>Germany</td>\n",
       "      <td>0.209763</td>\n",
       "      <td>-1.160614</td>\n",
       "      <td>1085.0</td>\n",
       "      <td>-0.796841</td>\n",
       "      <td>0.048080</td>\n",
       "      <td>-0.185510</td>\n",
       "      <td>0.313485</td>\n",
       "      <td>115281.25</td>\n",
       "      <td>-0.074981</td>\n",
       "      <td>-14918.75</td>\n",
       "      <td>2014-10-01</td>\n",
       "      <td>10</td>\n",
       "      <td>October</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>Midmarket</td>\n",
       "      <td>Germany</td>\n",
       "      <td>0.209763</td>\n",
       "      <td>-1.160614</td>\n",
       "      <td>1175.0</td>\n",
       "      <td>-0.796841</td>\n",
       "      <td>-0.756733</td>\n",
       "      <td>-0.649929</td>\n",
       "      <td>-0.457874</td>\n",
       "      <td>14981.25</td>\n",
       "      <td>-0.656417</td>\n",
       "      <td>3231.25</td>\n",
       "      <td>2014-10-01</td>\n",
       "      <td>10</td>\n",
       "      <td>October</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>Channel Partners</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>0.209763</td>\n",
       "      <td>-1.160614</td>\n",
       "      <td>914.0</td>\n",
       "      <td>-0.796841</td>\n",
       "      <td>-0.778682</td>\n",
       "      <td>-0.676130</td>\n",
       "      <td>-0.501390</td>\n",
       "      <td>9322.80</td>\n",
       "      <td>-0.700635</td>\n",
       "      <td>6580.80</td>\n",
       "      <td>2014-12-01</td>\n",
       "      <td>12</td>\n",
       "      <td>December</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>700 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Segment                   Country   Product  Discount Band  \\\n",
       "0          Government                   Germany -1.045228       1.874616   \n",
       "1          Government                   Germany  0.209763       1.874616   \n",
       "2          Government                    Canada  0.209763       1.874616   \n",
       "3          Government                   Germany  0.209763       1.874616   \n",
       "4          Government                   Germany  1.464753       1.874616   \n",
       "..                ...                       ...       ...            ...   \n",
       "695         Midmarket                    Canada  0.209763      -1.160614   \n",
       "696         Midmarket                    Canada  0.209763      -1.160614   \n",
       "697        Enterprise                   Germany  0.209763      -1.160614   \n",
       "698         Midmarket                   Germany  0.209763      -1.160614   \n",
       "699  Channel Partners  United States of America  0.209763      -1.160614   \n",
       "\n",
       "     Units Sold  Manufacturing Price  Sale Price  Gross Sales  Discounts  \\\n",
       "0        1513.0            -0.861342    1.694287     1.364884  -0.573087   \n",
       "1        1006.0            -0.796841    1.694287     0.666484  -0.573087   \n",
       "2        1725.0            -0.796841    1.694287     1.656917  -0.573087   \n",
       "3        1513.0            -0.796841    1.694287     1.364884  -0.573087   \n",
       "4        1006.0             0.216751    1.694287     0.666484  -0.573087   \n",
       "..          ...                  ...         ...          ...        ...   \n",
       "695      1614.0            -0.796841   -0.756733    -0.624012  -0.414828   \n",
       "696      2559.0            -0.796841   -0.756733    -0.568223  -0.322167   \n",
       "697      1085.0            -0.796841    0.048080    -0.185510   0.313485   \n",
       "698      1175.0            -0.796841   -0.756733    -0.649929  -0.457874   \n",
       "699       914.0            -0.796841   -0.778682    -0.676130  -0.501390   \n",
       "\n",
       "         Sales      COGS     Profit       Date  Month Number Month Name  Year  \n",
       "0    529550.00  1.216891  136170.00 2014-12-01            12   December  2014  \n",
       "1    352100.00  0.569826   90540.00 2014-06-01             6       June  2014  \n",
       "2    603750.00  1.487458  155250.00 2013-11-01            11   November  2013  \n",
       "3    529550.00  1.216891  136170.00 2014-12-01            12   December  2014  \n",
       "4    352100.00  0.569826   90540.00 2014-06-01             6       June  2014  \n",
       "..         ...       ...        ...        ...           ...        ...   ...  \n",
       "695   20578.50 -0.634868    4438.50 2014-04-01             4      April  2014  \n",
       "696   32627.25 -0.588481    7037.25 2014-08-01             8     August  2014  \n",
       "697  115281.25 -0.074981  -14918.75 2014-10-01            10    October  2014  \n",
       "698   14981.25 -0.656417    3231.25 2014-10-01            10    October  2014  \n",
       "699    9322.80 -0.700635    6580.80 2014-12-01            12   December  2014  \n",
       "\n",
       "[700 rows x 16 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df  = pd.read_excel(r\"C:\\Users\\Paxerahealth\\Downloads\\test.xlsx\")\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "product_encoded = LabelEncoder()\n",
    "product_encoded.fit(df['Product'])\n",
    "df['Product'] = product_encoded.transform(df['Product'])\n",
    "\n",
    "discount_band_encoded = LabelEncoder()\n",
    "discount_band_encoded.fit(df['Discount Band'])\n",
    "df['Discount Band'] = discount_band_encoded.transform(df['Discount Band'])\n",
    "\n",
    "#  Normalize the data\n",
    "scaler = StandardScaler()\n",
    "df[['Product', 'Discount Band', 'Manufacturing Price', 'Sale Price', 'Gross Sales', 'Discounts', 'COGS']] = scaler.fit_transform(df[['Product', 'Discount Band', 'Manufacturing Price', 'Sale Price', 'Gross Sales', 'Discounts', 'COGS']])\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = df[['Product' , 'Discount Band','Manufacturing Price','Sale Price' ,  'Gross Sales',  'Discounts', 'COGS']].to_numpy().tolist()\n",
    "target = df['Profit'].to_numpy().tolist()   \n",
    "\n",
    "\n",
    "data_length = len(data)\n",
    "\n",
    "data_train = torch.tensor(data[:int(data_length*0.8)], device=device , requires_grad=True)\n",
    "data_test = torch.tensor(data[int(data_length*0.8):], device=device , requires_grad=True)\n",
    "target_train = torch.tensor(target[:int(data_length*0.8)], device=device , requires_grad=True).unsqueeze(1)\n",
    "target_test = torch.tensor(target[int(data_length*0.8):], device=device , requires_grad=True).unsqueeze(1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(7,1), requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.randn(1), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matmul(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = LinearRegression()\n",
    "loss_fn = nn.MSELoss()\n",
    "# // Adjust the learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train_loss 2828229120.0 - test loss 625555648.0\n",
      "Epoch 1000 - Train_loss 68828496.0 - test loss 95773904.0\n",
      "Epoch 2000 - Train_loss 8806851.0 - test loss 12254570.0\n",
      "Epoch 3000 - Train_loss 1126818.5 - test loss 1567975.0\n",
      "Epoch 4000 - Train_loss 144184.34375 - test loss 200640.6875\n",
      "Epoch 5000 - Train_loss 18465.78125 - test loss 25695.384765625\n",
      "Epoch 6000 - Train_loss 2369.174072265625 - test loss 3297.21240234375\n",
      "Epoch 7000 - Train_loss 304.23028564453125 - test loss 423.28656005859375\n",
      "Epoch 8000 - Train_loss 39.42954635620117 - test loss 54.76101303100586\n",
      "Epoch 9000 - Train_loss 4.9545698165893555 - test loss 6.9530415534973145\n",
      "Epoch 10000 - Train_loss 0.5675973892211914 - test loss 0.8035590052604675\n",
      "Epoch 11000 - Train_loss 0.43488815426826477 - test loss 0.6132781505584717\n",
      "Epoch 12000 - Train_loss 0.4347723126411438 - test loss 0.6132725477218628\n",
      "Epoch 13000 - Train_loss 0.43478119373321533 - test loss 0.6131641864776611\n",
      "Epoch 14000 - Train_loss 0.4346874952316284 - test loss 0.6132650971412659\n",
      "Epoch 15000 - Train_loss 0.43463945388793945 - test loss 0.6132725477218628\n",
      "Epoch 16000 - Train_loss 0.43455934524536133 - test loss 0.6132725477218628\n",
      "Epoch 17000 - Train_loss 0.43480709195137024 - test loss 0.6132878065109253\n",
      "Epoch 18000 - Train_loss 0.43488040566444397 - test loss 0.6132781505584717\n",
      "Epoch 19000 - Train_loss 0.43396902084350586 - test loss 0.6132855415344238\n",
      "Epoch 20000 - Train_loss 0.43482765555381775 - test loss 0.6132781505584717\n",
      "Epoch 21000 - Train_loss 0.43497100472450256 - test loss 0.6132781505584717\n",
      "Epoch 22000 - Train_loss 0.4346093237400055 - test loss 0.6132985949516296\n",
      "Epoch 23000 - Train_loss 0.43497100472450256 - test loss 0.6132781505584717\n",
      "Epoch 24000 - Train_loss 0.43463173508644104 - test loss 0.6132855415344238\n",
      "Epoch 25000 - Train_loss 0.43459030985832214 - test loss 0.6131641864776611\n",
      "Epoch 26000 - Train_loss 0.4347723126411438 - test loss 0.6132725477218628\n",
      "Epoch 27000 - Train_loss 0.43488815426826477 - test loss 0.6132781505584717\n",
      "Epoch 28000 - Train_loss 0.43459030985832214 - test loss 0.6131641864776611\n",
      "Epoch 29000 - Train_loss 0.4347475469112396 - test loss 0.6132781505584717\n",
      "Epoch 30000 - Train_loss 0.43497100472450256 - test loss 0.6132781505584717\n",
      "Epoch 31000 - Train_loss 0.4347475469112396 - test loss 0.6132781505584717\n",
      "Epoch 32000 - Train_loss 0.43497100472450256 - test loss 0.6132781505584717\n",
      "Epoch 33000 - Train_loss 0.4348580539226532 - test loss 0.6132781505584717\n",
      "Epoch 34000 - Train_loss 0.4348888695240021 - test loss 0.6132781505584717\n",
      "Epoch 35000 - Train_loss 0.434990793466568 - test loss 0.6132781505584717\n",
      "Epoch 36000 - Train_loss 0.4347475469112396 - test loss 0.6132781505584717\n",
      "Epoch 37000 - Train_loss 0.43497100472450256 - test loss 0.6132781505584717\n",
      "Epoch 38000 - Train_loss 0.43474483489990234 - test loss 0.6132781505584717\n",
      "Epoch 39000 - Train_loss 0.4347475469112396 - test loss 0.6132781505584717\n",
      "Epoch 40000 - Train_loss 0.4348551332950592 - test loss 0.6132650971412659\n",
      "Epoch 41000 - Train_loss 0.4347769618034363 - test loss 0.6133138537406921\n",
      "Epoch 42000 - Train_loss 0.43505579233169556 - test loss 0.6132781505584717\n",
      "Epoch 43000 - Train_loss 0.4347483217716217 - test loss 0.6132781505584717\n",
      "Epoch 44000 - Train_loss 0.43488815426826477 - test loss 0.6131772398948669\n",
      "Epoch 45000 - Train_loss 0.43488815426826477 - test loss 0.6132781505584717\n",
      "Epoch 46000 - Train_loss 0.4347475469112396 - test loss 0.6132781505584717\n",
      "Epoch 47000 - Train_loss 0.4347769618034363 - test loss 0.6133138537406921\n",
      "Epoch 48000 - Train_loss 0.43459030985832214 - test loss 0.6131715774536133\n",
      "Epoch 49000 - Train_loss 0.4347723126411438 - test loss 0.6132725477218628\n",
      "Epoch 50000 - Train_loss 0.43488815426826477 - test loss 0.6132781505584717\n",
      "Epoch 51000 - Train_loss 0.43505579233169556 - test loss 0.6132781505584717\n",
      "Epoch 52000 - Train_loss 0.43472230434417725 - test loss 0.6132725477218628\n",
      "Epoch 53000 - Train_loss 0.43457457423210144 - test loss 0.6132985949516296\n",
      "Epoch 54000 - Train_loss 0.43488815426826477 - test loss 0.6131772398948669\n",
      "Epoch 55000 - Train_loss 0.43497100472450256 - test loss 0.6132781505584717\n",
      "Epoch 56000 - Train_loss 0.43488040566444397 - test loss 0.6132781505584717\n",
      "Epoch 57000 - Train_loss 0.43557658791542053 - test loss 0.6132650971412659\n",
      "Epoch 58000 - Train_loss 0.4347723126411438 - test loss 0.6132725477218628\n",
      "Epoch 59000 - Train_loss 0.43488815426826477 - test loss 0.6132781505584717\n",
      "Epoch 60000 - Train_loss 0.43505579233169556 - test loss 0.6132781505584717\n",
      "Epoch 61000 - Train_loss 0.43474483489990234 - test loss 0.6132855415344238\n",
      "Epoch 62000 - Train_loss 0.4346093237400055 - test loss 0.6132985949516296\n",
      "Epoch 63000 - Train_loss 0.43474483489990234 - test loss 0.6132855415344238\n",
      "Epoch 64000 - Train_loss 0.4347475469112396 - test loss 0.6131772398948669\n",
      "Epoch 65000 - Train_loss 0.4348580539226532 - test loss 0.6132650971412659\n",
      "Epoch 66000 - Train_loss 0.4348580539226532 - test loss 0.6132781505584717\n",
      "Epoch 67000 - Train_loss 0.4347475469112396 - test loss 0.6131772398948669\n",
      "Epoch 68000 - Train_loss 0.43474483489990234 - test loss 0.6132855415344238\n",
      "Epoch 69000 - Train_loss 0.43474483489990234 - test loss 0.6137267351150513\n",
      "Epoch 70000 - Train_loss 0.4347475469112396 - test loss 0.6131772398948669\n",
      "Epoch 71000 - Train_loss 0.43493613600730896 - test loss 0.6132781505584717\n",
      "Epoch 72000 - Train_loss 0.43455934524536133 - test loss 0.6132725477218628\n",
      "Epoch 73000 - Train_loss 0.4347475469112396 - test loss 0.6132781505584717\n",
      "Epoch 74000 - Train_loss 0.43488815426826477 - test loss 0.6132781505584717\n",
      "Epoch 75000 - Train_loss 0.43488815426826477 - test loss 0.6131772398948669\n",
      "Epoch 76000 - Train_loss 0.43461740016937256 - test loss 0.6132855415344238\n",
      "Epoch 77000 - Train_loss 0.43497100472450256 - test loss 0.6132781505584717\n",
      "Epoch 78000 - Train_loss 0.4347723126411438 - test loss 0.6132650971412659\n",
      "Epoch 79000 - Train_loss 0.4348399043083191 - test loss 0.6131641864776611\n",
      "Epoch 80000 - Train_loss 0.4347475469112396 - test loss 0.6132781505584717\n",
      "Epoch 81000 - Train_loss 0.43526843190193176 - test loss 0.6132781505584717\n",
      "Epoch 82000 - Train_loss 0.43475794792175293 - test loss 0.6131715774536133\n",
      "Epoch 83000 - Train_loss 0.4347723126411438 - test loss 0.6132725477218628\n",
      "Epoch 84000 - Train_loss 0.4349716901779175 - test loss 0.6132781505584717\n",
      "Epoch 85000 - Train_loss 0.434604674577713 - test loss 0.6132725477218628\n",
      "Epoch 86000 - Train_loss 0.4347475469112396 - test loss 0.6132781505584717\n",
      "Epoch 87000 - Train_loss 0.43463945388793945 - test loss 0.6132725477218628\n",
      "Epoch 88000 - Train_loss 0.43497100472450256 - test loss 0.6132650971412659\n",
      "Epoch 89000 - Train_loss 0.43485331535339355 - test loss 0.6132781505584717\n",
      "Epoch 90000 - Train_loss 0.434604674577713 - test loss 0.6132650971412659\n",
      "Epoch 91000 - Train_loss 0.4342486560344696 - test loss 0.6131772398948669\n",
      "Epoch 92000 - Train_loss 0.4346722662448883 - test loss 0.6132650971412659\n",
      "Epoch 93000 - Train_loss 0.43497100472450256 - test loss 0.6132781505584717\n",
      "Epoch 94000 - Train_loss 0.4347769618034363 - test loss 0.6133138537406921\n",
      "Epoch 95000 - Train_loss 0.4348888695240021 - test loss 0.6132781505584717\n",
      "Epoch 96000 - Train_loss 0.43457457423210144 - test loss 0.6132985949516296\n",
      "Epoch 97000 - Train_loss 0.43474483489990234 - test loss 0.6132855415344238\n",
      "Epoch 98000 - Train_loss 0.4347475469112396 - test loss 0.6132781505584717\n",
      "Epoch 99000 - Train_loss 0.4347475469112396 - test loss 0.6131772398948669\n",
      "Epoch 100000 - Train_loss 0.43497562408447266 - test loss 0.6132650971412659\n",
      "Epoch 101000 - Train_loss 0.43463945388793945 - test loss 0.6132725477218628\n",
      "Epoch 102000 - Train_loss 0.4348350763320923 - test loss 0.6132781505584717\n",
      "Epoch 103000 - Train_loss 0.4347475469112396 - test loss 0.6131772398948669\n",
      "Epoch 104000 - Train_loss 0.43497100472450256 - test loss 0.6132781505584717\n",
      "Epoch 105000 - Train_loss 0.4347483217716217 - test loss 0.6132781505584717\n",
      "Epoch 106000 - Train_loss 0.43488994240760803 - test loss 0.6132725477218628\n",
      "Epoch 107000 - Train_loss 0.43474483489990234 - test loss 0.6132650971412659\n",
      "Epoch 108000 - Train_loss 0.4348303973674774 - test loss 0.6132781505584717\n",
      "Epoch 109000 - Train_loss 0.43475794792175293 - test loss 0.6131715774536133\n",
      "Epoch 110000 - Train_loss 0.43480709195137024 - test loss 0.6132725477218628\n",
      "Epoch 111000 - Train_loss 0.43463945388793945 - test loss 0.6132725477218628\n",
      "Epoch 112000 - Train_loss 0.43463945388793945 - test loss 0.6131715774536133\n",
      "Epoch 113000 - Train_loss 0.4348580539226532 - test loss 0.6132650971412659\n",
      "Epoch 114000 - Train_loss 0.4348551332950592 - test loss 0.6132650971412659\n",
      "Epoch 115000 - Train_loss 0.4347475469112396 - test loss 0.6132781505584717\n",
      "Epoch 116000 - Train_loss 0.434604674577713 - test loss 0.6132650971412659\n",
      "Epoch 117000 - Train_loss 0.4347723126411438 - test loss 0.6132725477218628\n",
      "Epoch 118000 - Train_loss 0.43480709195137024 - test loss 0.6131715774536133\n",
      "Epoch 119000 - Train_loss 0.43474218249320984 - test loss 0.6132985949516296\n",
      "Epoch 120000 - Train_loss 0.4346874952316284 - test loss 0.6132650971412659\n",
      "Epoch 121000 - Train_loss 0.4349682033061981 - test loss 0.6132650971412659\n",
      "Epoch 122000 - Train_loss 0.43497100472450256 - test loss 0.6132781505584717\n",
      "Epoch 123000 - Train_loss 0.4347895085811615 - test loss 0.6132650971412659\n",
      "Epoch 124000 - Train_loss 0.4347475469112396 - test loss 0.6131772398948669\n",
      "Epoch 125000 - Train_loss 0.4347475469112396 - test loss 0.6132781505584717\n",
      "Epoch 126000 - Train_loss 0.4347475469112396 - test loss 0.6131772398948669\n",
      "Epoch 127000 - Train_loss 0.434990793466568 - test loss 0.6132781505584717\n",
      "Epoch 128000 - Train_loss 0.4348580539226532 - test loss 0.6132650971412659\n",
      "Epoch 129000 - Train_loss 0.4347475469112396 - test loss 0.6131772398948669\n",
      "Epoch 130000 - Train_loss 0.43497100472450256 - test loss 0.6132781505584717\n",
      "Epoch 131000 - Train_loss 0.43480709195137024 - test loss 0.6132725477218628\n",
      "Epoch 132000 - Train_loss 0.4347522556781769 - test loss 0.6137267351150513\n",
      "Epoch 133000 - Train_loss 0.43497100472450256 - test loss 0.6132650971412659\n",
      "Epoch 134000 - Train_loss 0.4347475469112396 - test loss 0.6132781505584717\n",
      "Epoch 135000 - Train_loss 0.4347769618034363 - test loss 0.6132878065109253\n",
      "Epoch 136000 - Train_loss 0.4347723126411438 - test loss 0.6132725477218628\n",
      "Epoch 137000 - Train_loss 0.4347475469112396 - test loss 0.6132781505584717\n",
      "Epoch 138000 - Train_loss 0.4348587691783905 - test loss 0.6132781505584717\n",
      "Epoch 139000 - Train_loss 0.43505579233169556 - test loss 0.6132781505584717\n",
      "Epoch 140000 - Train_loss 0.43474483489990234 - test loss 0.6132855415344238\n",
      "Epoch 141000 - Train_loss 0.43497100472450256 - test loss 0.6132781505584717\n",
      "Epoch 142000 - Train_loss 0.4348580539226532 - test loss 0.6132650971412659\n",
      "Epoch 143000 - Train_loss 0.4347475469112396 - test loss 0.6131772398948669\n",
      "Epoch 144000 - Train_loss 0.43480709195137024 - test loss 0.6131868362426758\n",
      "Epoch 145000 - Train_loss 0.4346874952316284 - test loss 0.6132650971412659\n",
      "Epoch 146000 - Train_loss 0.43488815426826477 - test loss 0.6132781505584717\n",
      "Epoch 147000 - Train_loss 0.43474483489990234 - test loss 0.6131846308708191\n",
      "Epoch 148000 - Train_loss 0.43488815426826477 - test loss 0.6132781505584717\n",
      "Epoch 149000 - Train_loss 0.43475794792175293 - test loss 0.6132725477218628\n",
      "Epoch 150000 - Train_loss 0.43488994240760803 - test loss 0.6132725477218628\n",
      "Epoch 151000 - Train_loss 0.43505579233169556 - test loss 0.6132781505584717\n",
      "Epoch 152000 - Train_loss 0.43488815426826477 - test loss 0.6132781505584717\n",
      "Epoch 153000 - Train_loss 0.4349716901779175 - test loss 0.6132650971412659\n",
      "Epoch 154000 - Train_loss 0.43497100472450256 - test loss 0.6132650971412659\n",
      "Epoch 155000 - Train_loss 0.43505579233169556 - test loss 0.6132781505584717\n",
      "Epoch 156000 - Train_loss 0.43463173508644104 - test loss 0.6132855415344238\n",
      "Epoch 157000 - Train_loss 0.43488815426826477 - test loss 0.6131772398948669\n",
      "Epoch 158000 - Train_loss 0.43505579233169556 - test loss 0.6132781505584717\n",
      "Epoch 159000 - Train_loss 0.4349716901779175 - test loss 0.6132650971412659\n",
      "Epoch 160000 - Train_loss 0.43474483489990234 - test loss 0.6137267351150513\n",
      "Epoch 161000 - Train_loss 0.4347475469112396 - test loss 0.6132781505584717\n",
      "Epoch 162000 - Train_loss 0.4347522556781769 - test loss 0.6132855415344238\n",
      "Epoch 163000 - Train_loss 0.43472230434417725 - test loss 0.6132725477218628\n",
      "Epoch 164000 - Train_loss 0.43497100472450256 - test loss 0.6132781505584717\n",
      "Epoch 165000 - Train_loss 0.43461740016937256 - test loss 0.6132855415344238\n",
      "Epoch 166000 - Train_loss 0.43497100472450256 - test loss 0.6132650971412659\n",
      "Epoch 167000 - Train_loss 0.4347723126411438 - test loss 0.6132725477218628\n",
      "Epoch 168000 - Train_loss 0.4346093237400055 - test loss 0.6137550473213196\n",
      "Epoch 169000 - Train_loss 0.4347475469112396 - test loss 0.6132781505584717\n",
      "Epoch 170000 - Train_loss 0.4346093237400055 - test loss 0.6133138537406921\n",
      "Epoch 171000 - Train_loss 0.43488815426826477 - test loss 0.6132650971412659\n",
      "Epoch 172000 - Train_loss 0.43482765555381775 - test loss 0.6132781505584717\n",
      "Epoch 173000 - Train_loss 0.43493613600730896 - test loss 0.6132781505584717\n",
      "Epoch 174000 - Train_loss 0.43488815426826477 - test loss 0.6132781505584717\n",
      "Epoch 175000 - Train_loss 0.43497100472450256 - test loss 0.6132650971412659\n",
      "Epoch 176000 - Train_loss 0.4348580539226532 - test loss 0.6132781505584717\n",
      "Epoch 177000 - Train_loss 0.43505650758743286 - test loss 0.6132781505584717\n",
      "Epoch 178000 - Train_loss 0.4348389506340027 - test loss 0.6131772398948669\n",
      "Epoch 179000 - Train_loss 0.43497100472450256 - test loss 0.6132650971412659\n",
      "Epoch 180000 - Train_loss 0.4347723126411438 - test loss 0.6132725477218628\n",
      "Epoch 181000 - Train_loss 0.43472230434417725 - test loss 0.6132725477218628\n",
      "Epoch 182000 - Train_loss 0.4347475469112396 - test loss 0.6131772398948669\n",
      "Epoch 183000 - Train_loss 0.4348580539226532 - test loss 0.6132650971412659\n",
      "Epoch 184000 - Train_loss 0.43474483489990234 - test loss 0.6132855415344238\n",
      "Epoch 185000 - Train_loss 0.43488815426826477 - test loss 0.6132781505584717\n",
      "Epoch 186000 - Train_loss 0.43510377407073975 - test loss 0.6132781505584717\n",
      "Epoch 187000 - Train_loss 0.4340819716453552 - test loss 0.6132650971412659\n",
      "Epoch 188000 - Train_loss 0.43488815426826477 - test loss 0.6131772398948669\n",
      "Epoch 189000 - Train_loss 0.4347475469112396 - test loss 0.6132781505584717\n",
      "Epoch 190000 - Train_loss 0.43488815426826477 - test loss 0.6132781505584717\n",
      "Epoch 191000 - Train_loss 0.43474483489990234 - test loss 0.6131772398948669\n",
      "Epoch 192000 - Train_loss 0.4347475469112396 - test loss 0.6131772398948669\n",
      "Epoch 193000 - Train_loss 0.43488815426826477 - test loss 0.6132781505584717\n",
      "Epoch 194000 - Train_loss 0.43497100472450256 - test loss 0.6132650971412659\n",
      "Epoch 195000 - Train_loss 0.43482765555381775 - test loss 0.6132781505584717\n",
      "Epoch 196000 - Train_loss 0.43488815426826477 - test loss 0.6132781505584717\n",
      "Epoch 197000 - Train_loss 0.43554648756980896 - test loss 0.6132650971412659\n",
      "Epoch 198000 - Train_loss 0.4348580539226532 - test loss 0.6132781505584717\n",
      "Epoch 199000 - Train_loss 0.4347475469112396 - test loss 0.6132781505584717\n",
      "Epoch 200000 - Train_loss 0.4347769618034363 - test loss 0.6132985949516296\n",
      "Epoch 201000 - Train_loss 0.43488815426826477 - test loss 0.6131772398948669\n",
      "Epoch 202000 - Train_loss 0.4347475469112396 - test loss 0.6132781505584717\n",
      "Epoch 203000 - Train_loss 0.4346093237400055 - test loss 0.6133138537406921\n",
      "Epoch 204000 - Train_loss 0.4348551332950592 - test loss 0.6132650971412659\n",
      "Epoch 205000 - Train_loss 0.43497100472450256 - test loss 0.6132650971412659\n",
      "Epoch 206000 - Train_loss 0.4347475469112396 - test loss 0.6131772398948669\n",
      "Epoch 207000 - Train_loss 0.43472230434417725 - test loss 0.6132650971412659\n",
      "Epoch 208000 - Train_loss 0.4348580539226532 - test loss 0.6132781505584717\n",
      "Epoch 209000 - Train_loss 0.4349716901779175 - test loss 0.6132650971412659\n",
      "Epoch 210000 - Train_loss 0.43497100472450256 - test loss 0.6132781505584717\n",
      "Epoch 211000 - Train_loss 0.43474218249320984 - test loss 0.6132911443710327\n",
      "Epoch 212000 - Train_loss 0.4348853826522827 - test loss 0.6132781505584717\n",
      "Epoch 213000 - Train_loss 0.43488815426826477 - test loss 0.6132781505584717\n",
      "Epoch 214000 - Train_loss 0.43480709195137024 - test loss 0.6132878065109253\n",
      "Epoch 215000 - Train_loss 0.4347895085811615 - test loss 0.6132725477218628\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     10\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data_train)\n\u001b[1;32m---> 11\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\GitHub_Repo\\torchstudy\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\GitHub_Repo\\torchstudy\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\GitHub_Repo\\torchstudy\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\GitHub_Repo\\torchstudy\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:3792\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3789\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m   3791\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[1;32m-> 3792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexpanded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3794\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoches = 10000000\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "epoches_list = []\n",
    "tr_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data_train)\n",
    "    loss = loss_fn(output, target_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    predict_test = model(data_test)\n",
    "    test_loss = loss_fn(predict_test, target_test)\n",
    "    \n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        epoches_list.append(epoch)\n",
    "        tr_losses.append( loss.item())\n",
    "        test_losses.append(test_loss.item())\n",
    "        print(f'Epoch {epoch} - Train_loss {loss.item()} - test loss {test_loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1cccd7f39e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data_test_numpy = data_test.cpu().detach().numpy()\n",
    "target_test_numpy  = target_test.cpu().detach().numpy()\n",
    "predict_test_numpy  = predict_test.cpu().detach().numpy()\n",
    "\n",
    "# type(data_test)\n",
    "\n",
    "\n",
    "# plt.scatter(data_test_numpy , predict_test_numpy , c = 'r' , label='Predicted')\n",
    "# plt.scatter(data_test_numpy , target_test_numpy , c = 'b' , label='Actual')\n",
    "# plt.scatter(target_test_numpy , predict_test_numpy , c = 'r', label='compare') \n",
    "plt.scatter(epoches_list , tr_losses , c = 'r' , s=1,  label='test_loss')\n",
    "plt.scatter(epoches_list , test_losses , c = 'g' , s=1,  label='test_loss')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([140, 1]), torch.Size([140, 7]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    predict2 = model(data_test)\n",
    "\n",
    "predict2.shape , data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model , 'model01.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_path = Path('model02.pth')\n",
    "model_path.mkdir(exist_ok=True, parents=True)\n",
    "model_name = 'model0101.pth'\n",
    "model_save = model_path / model_name\n",
    "torch.save(model.state_dict(), model_save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-7.2790e-03],\n",
       "                      [-2.1166e-03],\n",
       "                      [-2.9883e-02],\n",
       "                      [ 1.5771e-01],\n",
       "                      [ 2.5407e+05],\n",
       "                      [-2.2947e+04],\n",
       "                      [-2.0371e+05]])),\n",
       "             ('bias', tensor([24133.9414]))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paxerahealth\\AppData\\Local\\Temp\\ipykernel_19568\\3383649782.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model2.load_state_dict(torch.load(model_save))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-7.2790e-03],\n",
       "                      [-2.1166e-03],\n",
       "                      [-2.9883e-02],\n",
       "                      [ 1.5771e-01],\n",
       "                      [ 2.5407e+05],\n",
       "                      [-2.2947e+04],\n",
       "                      [-2.0371e+05]])),\n",
       "             ('bias', tensor([24133.9414]))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = LinearRegression()\n",
    "model2.load_state_dict(torch.load(model_save))\n",
    "# model2.eval()\n",
    "model2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model2(data_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
