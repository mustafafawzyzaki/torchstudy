{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchmetrics import MeanSquaredError\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "define-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.output(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "initialize-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, optimizer, and metrics\n",
    "input_dim = 15\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "model = RegressionModel(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "mse = MeanSquaredError()\n",
    "writer = SummaryWriter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dummy-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data for training (example: regression)\n",
    "x_train = torch.randn(100, input_dim)\n",
    "y_train = torch.randn(100, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "train-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100000], Loss: 1.0135, MSE: 1.0135\n",
      "Epoch [200/100000], Loss: 0.9836, MSE: 0.9836\n",
      "Epoch [300/100000], Loss: 0.9673, MSE: 0.9673\n",
      "Epoch [400/100000], Loss: 0.9583, MSE: 0.9583\n",
      "Epoch [500/100000], Loss: 0.9532, MSE: 0.9532\n",
      "Epoch [600/100000], Loss: 0.9502, MSE: 0.9502\n",
      "Epoch [700/100000], Loss: 0.9485, MSE: 0.9485\n",
      "Epoch [800/100000], Loss: 0.9474, MSE: 0.9474\n",
      "Epoch [900/100000], Loss: 0.9466, MSE: 0.9466\n",
      "Epoch [1000/100000], Loss: 0.9461, MSE: 0.9461\n",
      "Epoch [1100/100000], Loss: 0.9456, MSE: 0.9456\n",
      "Epoch [1200/100000], Loss: 0.9452, MSE: 0.9452\n",
      "Epoch [1300/100000], Loss: 0.9449, MSE: 0.9449\n",
      "Epoch [1400/100000], Loss: 0.9446, MSE: 0.9446\n",
      "Epoch [1500/100000], Loss: 0.9442, MSE: 0.9442\n",
      "Epoch [1600/100000], Loss: 0.9439, MSE: 0.9439\n",
      "Epoch [1700/100000], Loss: 0.9436, MSE: 0.9436\n",
      "Epoch [1800/100000], Loss: 0.9433, MSE: 0.9433\n",
      "Epoch [1900/100000], Loss: 0.9429, MSE: 0.9429\n",
      "Epoch [2000/100000], Loss: 0.9426, MSE: 0.9426\n",
      "Epoch [2100/100000], Loss: 0.9423, MSE: 0.9423\n",
      "Epoch [2200/100000], Loss: 0.9419, MSE: 0.9419\n",
      "Epoch [2300/100000], Loss: 0.9416, MSE: 0.9416\n",
      "Epoch [2400/100000], Loss: 0.9412, MSE: 0.9412\n",
      "Epoch [2500/100000], Loss: 0.9408, MSE: 0.9408\n",
      "Epoch [2600/100000], Loss: 0.9405, MSE: 0.9405\n",
      "Epoch [2700/100000], Loss: 0.9401, MSE: 0.9401\n",
      "Epoch [2800/100000], Loss: 0.9397, MSE: 0.9397\n",
      "Epoch [2900/100000], Loss: 0.9393, MSE: 0.9393\n",
      "Epoch [3000/100000], Loss: 0.9388, MSE: 0.9388\n",
      "Epoch [3100/100000], Loss: 0.9384, MSE: 0.9384\n",
      "Epoch [3200/100000], Loss: 0.9379, MSE: 0.9379\n",
      "Epoch [3300/100000], Loss: 0.9374, MSE: 0.9374\n",
      "Epoch [3400/100000], Loss: 0.9369, MSE: 0.9369\n",
      "Epoch [3500/100000], Loss: 0.9364, MSE: 0.9364\n",
      "Epoch [3600/100000], Loss: 0.9359, MSE: 0.9359\n",
      "Epoch [3700/100000], Loss: 0.9354, MSE: 0.9354\n",
      "Epoch [3800/100000], Loss: 0.9348, MSE: 0.9348\n",
      "Epoch [3900/100000], Loss: 0.9343, MSE: 0.9343\n",
      "Epoch [4000/100000], Loss: 0.9337, MSE: 0.9337\n",
      "Epoch [4100/100000], Loss: 0.9332, MSE: 0.9332\n",
      "Epoch [4200/100000], Loss: 0.9326, MSE: 0.9326\n",
      "Epoch [4300/100000], Loss: 0.9320, MSE: 0.9320\n",
      "Epoch [4400/100000], Loss: 0.9313, MSE: 0.9313\n",
      "Epoch [4500/100000], Loss: 0.9307, MSE: 0.9307\n",
      "Epoch [4600/100000], Loss: 0.9300, MSE: 0.9300\n",
      "Epoch [4700/100000], Loss: 0.9293, MSE: 0.9293\n",
      "Epoch [4800/100000], Loss: 0.9287, MSE: 0.9287\n",
      "Epoch [4900/100000], Loss: 0.9280, MSE: 0.9280\n",
      "Epoch [5000/100000], Loss: 0.9272, MSE: 0.9272\n",
      "Epoch [5100/100000], Loss: 0.9265, MSE: 0.9265\n",
      "Epoch [5200/100000], Loss: 0.9257, MSE: 0.9257\n",
      "Epoch [5300/100000], Loss: 0.9249, MSE: 0.9249\n",
      "Epoch [5400/100000], Loss: 0.9241, MSE: 0.9241\n",
      "Epoch [5500/100000], Loss: 0.9233, MSE: 0.9233\n",
      "Epoch [5600/100000], Loss: 0.9224, MSE: 0.9224\n",
      "Epoch [5700/100000], Loss: 0.9215, MSE: 0.9215\n",
      "Epoch [5800/100000], Loss: 0.9205, MSE: 0.9205\n",
      "Epoch [5900/100000], Loss: 0.9195, MSE: 0.9195\n",
      "Epoch [6000/100000], Loss: 0.9185, MSE: 0.9185\n",
      "Epoch [6100/100000], Loss: 0.9174, MSE: 0.9174\n",
      "Epoch [6200/100000], Loss: 0.9162, MSE: 0.9162\n",
      "Epoch [6300/100000], Loss: 0.9151, MSE: 0.9151\n",
      "Epoch [6400/100000], Loss: 0.9138, MSE: 0.9138\n",
      "Epoch [6500/100000], Loss: 0.9126, MSE: 0.9126\n",
      "Epoch [6600/100000], Loss: 0.9113, MSE: 0.9113\n",
      "Epoch [6700/100000], Loss: 0.9100, MSE: 0.9100\n",
      "Epoch [6800/100000], Loss: 0.9086, MSE: 0.9086\n",
      "Epoch [6900/100000], Loss: 0.9071, MSE: 0.9071\n",
      "Epoch [7000/100000], Loss: 0.9055, MSE: 0.9055\n",
      "Epoch [7100/100000], Loss: 0.9038, MSE: 0.9038\n",
      "Epoch [7200/100000], Loss: 0.9020, MSE: 0.9020\n",
      "Epoch [7300/100000], Loss: 0.9000, MSE: 0.9000\n",
      "Epoch [7400/100000], Loss: 0.8980, MSE: 0.8980\n",
      "Epoch [7500/100000], Loss: 0.8958, MSE: 0.8958\n",
      "Epoch [7600/100000], Loss: 0.8935, MSE: 0.8935\n",
      "Epoch [7700/100000], Loss: 0.8909, MSE: 0.8909\n",
      "Epoch [7800/100000], Loss: 0.8881, MSE: 0.8881\n",
      "Epoch [7900/100000], Loss: 0.8853, MSE: 0.8853\n",
      "Epoch [8000/100000], Loss: 0.8822, MSE: 0.8822\n",
      "Epoch [8100/100000], Loss: 0.8790, MSE: 0.8790\n",
      "Epoch [8200/100000], Loss: 0.8756, MSE: 0.8756\n",
      "Epoch [8300/100000], Loss: 0.8721, MSE: 0.8721\n",
      "Epoch [8400/100000], Loss: 0.8683, MSE: 0.8683\n",
      "Epoch [8500/100000], Loss: 0.8641, MSE: 0.8641\n",
      "Epoch [8600/100000], Loss: 0.8597, MSE: 0.8597\n",
      "Epoch [8700/100000], Loss: 0.8549, MSE: 0.8549\n",
      "Epoch [8800/100000], Loss: 0.8499, MSE: 0.8499\n",
      "Epoch [8900/100000], Loss: 0.8448, MSE: 0.8448\n",
      "Epoch [9000/100000], Loss: 0.8397, MSE: 0.8397\n",
      "Epoch [9100/100000], Loss: 0.8346, MSE: 0.8346\n",
      "Epoch [9200/100000], Loss: 0.8291, MSE: 0.8291\n",
      "Epoch [9300/100000], Loss: 0.8232, MSE: 0.8232\n",
      "Epoch [9400/100000], Loss: 0.8168, MSE: 0.8168\n",
      "Epoch [9500/100000], Loss: 0.8106, MSE: 0.8106\n",
      "Epoch [9600/100000], Loss: 0.8043, MSE: 0.8043\n",
      "Epoch [9700/100000], Loss: 0.7980, MSE: 0.7980\n",
      "Epoch [9800/100000], Loss: 0.7915, MSE: 0.7915\n",
      "Epoch [9900/100000], Loss: 0.7848, MSE: 0.7848\n",
      "Epoch [10000/100000], Loss: 0.7780, MSE: 0.7780\n",
      "Epoch [10100/100000], Loss: 0.7712, MSE: 0.7712\n",
      "Epoch [10200/100000], Loss: 0.7643, MSE: 0.7643\n",
      "Epoch [10300/100000], Loss: 0.7573, MSE: 0.7573\n",
      "Epoch [10400/100000], Loss: 0.7500, MSE: 0.7500\n",
      "Epoch [10500/100000], Loss: 0.7429, MSE: 0.7429\n",
      "Epoch [10600/100000], Loss: 0.7355, MSE: 0.7355\n",
      "Epoch [10700/100000], Loss: 0.7281, MSE: 0.7281\n",
      "Epoch [10800/100000], Loss: 0.7210, MSE: 0.7210\n",
      "Epoch [10900/100000], Loss: 0.7139, MSE: 0.7139\n",
      "Epoch [11000/100000], Loss: 0.7071, MSE: 0.7071\n",
      "Epoch [11100/100000], Loss: 0.7003, MSE: 0.7003\n",
      "Epoch [11200/100000], Loss: 0.6936, MSE: 0.6936\n",
      "Epoch [11300/100000], Loss: 0.6869, MSE: 0.6869\n",
      "Epoch [11400/100000], Loss: 0.6802, MSE: 0.6802\n",
      "Epoch [11500/100000], Loss: 0.6737, MSE: 0.6737\n",
      "Epoch [11600/100000], Loss: 0.6671, MSE: 0.6671\n",
      "Epoch [11700/100000], Loss: 0.6605, MSE: 0.6605\n",
      "Epoch [11800/100000], Loss: 0.6538, MSE: 0.6538\n",
      "Epoch [11900/100000], Loss: 0.6469, MSE: 0.6469\n",
      "Epoch [12000/100000], Loss: 0.6398, MSE: 0.6398\n",
      "Epoch [12100/100000], Loss: 0.6328, MSE: 0.6328\n",
      "Epoch [12200/100000], Loss: 0.6259, MSE: 0.6259\n",
      "Epoch [12300/100000], Loss: 0.6192, MSE: 0.6192\n",
      "Epoch [12400/100000], Loss: 0.6125, MSE: 0.6125\n",
      "Epoch [12500/100000], Loss: 0.6058, MSE: 0.6058\n",
      "Epoch [12600/100000], Loss: 0.5992, MSE: 0.5992\n",
      "Epoch [12700/100000], Loss: 0.5923, MSE: 0.5923\n",
      "Epoch [12800/100000], Loss: 0.5853, MSE: 0.5853\n",
      "Epoch [12900/100000], Loss: 0.5784, MSE: 0.5784\n",
      "Epoch [13000/100000], Loss: 0.5714, MSE: 0.5714\n",
      "Epoch [13100/100000], Loss: 0.5641, MSE: 0.5641\n",
      "Epoch [13200/100000], Loss: 0.5562, MSE: 0.5562\n",
      "Epoch [13300/100000], Loss: 0.5476, MSE: 0.5476\n",
      "Epoch [13400/100000], Loss: 0.5390, MSE: 0.5390\n",
      "Epoch [13500/100000], Loss: 0.5305, MSE: 0.5305\n",
      "Epoch [13600/100000], Loss: 0.5218, MSE: 0.5218\n",
      "Epoch [13700/100000], Loss: 0.5130, MSE: 0.5130\n",
      "Epoch [13800/100000], Loss: 0.5043, MSE: 0.5043\n",
      "Epoch [13900/100000], Loss: 0.4954, MSE: 0.4954\n",
      "Epoch [14000/100000], Loss: 0.4864, MSE: 0.4864\n",
      "Epoch [14100/100000], Loss: 0.4772, MSE: 0.4772\n",
      "Epoch [14200/100000], Loss: 0.4685, MSE: 0.4685\n",
      "Epoch [14300/100000], Loss: 0.4601, MSE: 0.4601\n",
      "Epoch [14400/100000], Loss: 0.4517, MSE: 0.4517\n",
      "Epoch [14500/100000], Loss: 0.4431, MSE: 0.4431\n",
      "Epoch [14600/100000], Loss: 0.4347, MSE: 0.4347\n",
      "Epoch [14700/100000], Loss: 0.4266, MSE: 0.4266\n",
      "Epoch [14800/100000], Loss: 0.4185, MSE: 0.4185\n",
      "Epoch [14900/100000], Loss: 0.4106, MSE: 0.4106\n",
      "Epoch [15000/100000], Loss: 0.4028, MSE: 0.4028\n",
      "Epoch [15100/100000], Loss: 0.3950, MSE: 0.3950\n",
      "Epoch [15200/100000], Loss: 0.3869, MSE: 0.3869\n",
      "Epoch [15300/100000], Loss: 0.3790, MSE: 0.3790\n",
      "Epoch [15400/100000], Loss: 0.3711, MSE: 0.3711\n",
      "Epoch [15500/100000], Loss: 0.3636, MSE: 0.3636\n",
      "Epoch [15600/100000], Loss: 0.3553, MSE: 0.3553\n",
      "Epoch [15700/100000], Loss: 0.3470, MSE: 0.3470\n",
      "Epoch [15800/100000], Loss: 0.3390, MSE: 0.3390\n",
      "Epoch [15900/100000], Loss: 0.3311, MSE: 0.3311\n",
      "Epoch [16000/100000], Loss: 0.3235, MSE: 0.3235\n",
      "Epoch [16100/100000], Loss: 0.3160, MSE: 0.3160\n",
      "Epoch [16200/100000], Loss: 0.3087, MSE: 0.3087\n",
      "Epoch [16300/100000], Loss: 0.3015, MSE: 0.3015\n",
      "Epoch [16400/100000], Loss: 0.2944, MSE: 0.2944\n",
      "Epoch [16500/100000], Loss: 0.2875, MSE: 0.2875\n",
      "Epoch [16600/100000], Loss: 0.2807, MSE: 0.2807\n",
      "Epoch [16700/100000], Loss: 0.2741, MSE: 0.2741\n",
      "Epoch [16800/100000], Loss: 0.2678, MSE: 0.2678\n",
      "Epoch [16900/100000], Loss: 0.2616, MSE: 0.2616\n",
      "Epoch [17000/100000], Loss: 0.2557, MSE: 0.2557\n",
      "Epoch [17100/100000], Loss: 0.2500, MSE: 0.2500\n",
      "Epoch [17200/100000], Loss: 0.2448, MSE: 0.2448\n",
      "Epoch [17300/100000], Loss: 0.2396, MSE: 0.2396\n",
      "Epoch [17400/100000], Loss: 0.2344, MSE: 0.2344\n",
      "Epoch [17500/100000], Loss: 0.2292, MSE: 0.2292\n",
      "Epoch [17600/100000], Loss: 0.2238, MSE: 0.2238\n",
      "Epoch [17700/100000], Loss: 0.2186, MSE: 0.2186\n",
      "Epoch [17800/100000], Loss: 0.2136, MSE: 0.2136\n",
      "Epoch [17900/100000], Loss: 0.2087, MSE: 0.2087\n",
      "Epoch [18000/100000], Loss: 0.2039, MSE: 0.2039\n",
      "Epoch [18100/100000], Loss: 0.1990, MSE: 0.1990\n",
      "Epoch [18200/100000], Loss: 0.1942, MSE: 0.1942\n",
      "Epoch [18300/100000], Loss: 0.1898, MSE: 0.1898\n",
      "Epoch [18400/100000], Loss: 0.1855, MSE: 0.1855\n",
      "Epoch [18500/100000], Loss: 0.1811, MSE: 0.1811\n",
      "Epoch [18600/100000], Loss: 0.1767, MSE: 0.1767\n",
      "Epoch [18700/100000], Loss: 0.1724, MSE: 0.1724\n",
      "Epoch [18800/100000], Loss: 0.1683, MSE: 0.1683\n",
      "Epoch [18900/100000], Loss: 0.1642, MSE: 0.1642\n",
      "Epoch [19000/100000], Loss: 0.1602, MSE: 0.1602\n",
      "Epoch [19100/100000], Loss: 0.1562, MSE: 0.1562\n",
      "Epoch [19200/100000], Loss: 0.1521, MSE: 0.1521\n",
      "Epoch [19300/100000], Loss: 0.1479, MSE: 0.1479\n",
      "Epoch [19400/100000], Loss: 0.1438, MSE: 0.1438\n",
      "Epoch [19500/100000], Loss: 0.1400, MSE: 0.1400\n",
      "Epoch [19600/100000], Loss: 0.1362, MSE: 0.1362\n",
      "Epoch [19700/100000], Loss: 0.1324, MSE: 0.1324\n",
      "Epoch [19800/100000], Loss: 0.1288, MSE: 0.1288\n",
      "Epoch [19900/100000], Loss: 0.1252, MSE: 0.1252\n",
      "Epoch [20000/100000], Loss: 0.1216, MSE: 0.1216\n",
      "Epoch [20100/100000], Loss: 0.1182, MSE: 0.1182\n",
      "Epoch [20200/100000], Loss: 0.1150, MSE: 0.1150\n",
      "Epoch [20300/100000], Loss: 0.1119, MSE: 0.1119\n",
      "Epoch [20400/100000], Loss: 0.1089, MSE: 0.1089\n",
      "Epoch [20500/100000], Loss: 0.1060, MSE: 0.1060\n",
      "Epoch [20600/100000], Loss: 0.1032, MSE: 0.1032\n",
      "Epoch [20700/100000], Loss: 0.1005, MSE: 0.1005\n",
      "Epoch [20800/100000], Loss: 0.0980, MSE: 0.0980\n",
      "Epoch [20900/100000], Loss: 0.0955, MSE: 0.0955\n",
      "Epoch [21000/100000], Loss: 0.0930, MSE: 0.0930\n",
      "Epoch [21100/100000], Loss: 0.0906, MSE: 0.0906\n",
      "Epoch [21200/100000], Loss: 0.0884, MSE: 0.0884\n",
      "Epoch [21300/100000], Loss: 0.0863, MSE: 0.0863\n",
      "Epoch [21400/100000], Loss: 0.0842, MSE: 0.0842\n",
      "Epoch [21500/100000], Loss: 0.0822, MSE: 0.0822\n",
      "Epoch [21600/100000], Loss: 0.0803, MSE: 0.0803\n",
      "Epoch [21700/100000], Loss: 0.0784, MSE: 0.0784\n",
      "Epoch [21800/100000], Loss: 0.0766, MSE: 0.0766\n",
      "Epoch [21900/100000], Loss: 0.0748, MSE: 0.0748\n",
      "Epoch [22000/100000], Loss: 0.0731, MSE: 0.0731\n",
      "Epoch [22100/100000], Loss: 0.0714, MSE: 0.0714\n",
      "Epoch [22200/100000], Loss: 0.0698, MSE: 0.0698\n",
      "Epoch [22300/100000], Loss: 0.0682, MSE: 0.0682\n",
      "Epoch [22400/100000], Loss: 0.0667, MSE: 0.0667\n",
      "Epoch [22500/100000], Loss: 0.0653, MSE: 0.0653\n",
      "Epoch [22600/100000], Loss: 0.0638, MSE: 0.0638\n",
      "Epoch [22700/100000], Loss: 0.0620, MSE: 0.0620\n",
      "Epoch [22800/100000], Loss: 0.0604, MSE: 0.0604\n",
      "Epoch [22900/100000], Loss: 0.0589, MSE: 0.0589\n",
      "Epoch [23000/100000], Loss: 0.0574, MSE: 0.0574\n",
      "Epoch [23100/100000], Loss: 0.0559, MSE: 0.0559\n",
      "Epoch [23200/100000], Loss: 0.0543, MSE: 0.0543\n",
      "Epoch [23300/100000], Loss: 0.0529, MSE: 0.0529\n",
      "Epoch [23400/100000], Loss: 0.0515, MSE: 0.0515\n",
      "Epoch [23500/100000], Loss: 0.0502, MSE: 0.0502\n",
      "Epoch [23600/100000], Loss: 0.0490, MSE: 0.0490\n",
      "Epoch [23700/100000], Loss: 0.0478, MSE: 0.0478\n",
      "Epoch [23800/100000], Loss: 0.0467, MSE: 0.0467\n",
      "Epoch [23900/100000], Loss: 0.0456, MSE: 0.0456\n",
      "Epoch [24000/100000], Loss: 0.0446, MSE: 0.0446\n",
      "Epoch [24100/100000], Loss: 0.0435, MSE: 0.0435\n",
      "Epoch [24200/100000], Loss: 0.0425, MSE: 0.0425\n",
      "Epoch [24300/100000], Loss: 0.0415, MSE: 0.0415\n",
      "Epoch [24400/100000], Loss: 0.0405, MSE: 0.0405\n",
      "Epoch [24500/100000], Loss: 0.0396, MSE: 0.0396\n",
      "Epoch [24600/100000], Loss: 0.0387, MSE: 0.0387\n",
      "Epoch [24700/100000], Loss: 0.0378, MSE: 0.0378\n",
      "Epoch [24800/100000], Loss: 0.0370, MSE: 0.0370\n",
      "Epoch [24900/100000], Loss: 0.0361, MSE: 0.0361\n",
      "Epoch [25000/100000], Loss: 0.0353, MSE: 0.0353\n",
      "Epoch [25100/100000], Loss: 0.0346, MSE: 0.0346\n",
      "Epoch [25200/100000], Loss: 0.0338, MSE: 0.0338\n",
      "Epoch [25300/100000], Loss: 0.0331, MSE: 0.0331\n",
      "Epoch [25400/100000], Loss: 0.0324, MSE: 0.0324\n",
      "Epoch [25500/100000], Loss: 0.0317, MSE: 0.0317\n",
      "Epoch [25600/100000], Loss: 0.0311, MSE: 0.0311\n",
      "Epoch [25700/100000], Loss: 0.0304, MSE: 0.0304\n",
      "Epoch [25800/100000], Loss: 0.0298, MSE: 0.0298\n",
      "Epoch [25900/100000], Loss: 0.0292, MSE: 0.0292\n",
      "Epoch [26000/100000], Loss: 0.0286, MSE: 0.0286\n",
      "Epoch [26100/100000], Loss: 0.0281, MSE: 0.0281\n",
      "Epoch [26200/100000], Loss: 0.0275, MSE: 0.0275\n",
      "Epoch [26300/100000], Loss: 0.0270, MSE: 0.0270\n",
      "Epoch [26400/100000], Loss: 0.0265, MSE: 0.0265\n",
      "Epoch [26500/100000], Loss: 0.0260, MSE: 0.0260\n",
      "Epoch [26600/100000], Loss: 0.0255, MSE: 0.0255\n",
      "Epoch [26700/100000], Loss: 0.0250, MSE: 0.0250\n",
      "Epoch [26800/100000], Loss: 0.0245, MSE: 0.0245\n",
      "Epoch [26900/100000], Loss: 0.0241, MSE: 0.0241\n",
      "Epoch [27000/100000], Loss: 0.0237, MSE: 0.0237\n",
      "Epoch [27100/100000], Loss: 0.0232, MSE: 0.0232\n",
      "Epoch [27200/100000], Loss: 0.0228, MSE: 0.0228\n",
      "Epoch [27300/100000], Loss: 0.0224, MSE: 0.0224\n",
      "Epoch [27400/100000], Loss: 0.0220, MSE: 0.0220\n",
      "Epoch [27500/100000], Loss: 0.0216, MSE: 0.0216\n",
      "Epoch [27600/100000], Loss: 0.0212, MSE: 0.0212\n",
      "Epoch [27700/100000], Loss: 0.0208, MSE: 0.0208\n",
      "Epoch [27800/100000], Loss: 0.0205, MSE: 0.0205\n",
      "Epoch [27900/100000], Loss: 0.0201, MSE: 0.0201\n",
      "Epoch [28000/100000], Loss: 0.0198, MSE: 0.0198\n",
      "Epoch [28100/100000], Loss: 0.0194, MSE: 0.0194\n",
      "Epoch [28200/100000], Loss: 0.0191, MSE: 0.0191\n",
      "Epoch [28300/100000], Loss: 0.0188, MSE: 0.0188\n",
      "Epoch [28400/100000], Loss: 0.0185, MSE: 0.0185\n",
      "Epoch [28500/100000], Loss: 0.0182, MSE: 0.0182\n",
      "Epoch [28600/100000], Loss: 0.0179, MSE: 0.0179\n",
      "Epoch [28700/100000], Loss: 0.0176, MSE: 0.0176\n",
      "Epoch [28800/100000], Loss: 0.0173, MSE: 0.0173\n",
      "Epoch [28900/100000], Loss: 0.0170, MSE: 0.0170\n",
      "Epoch [29000/100000], Loss: 0.0168, MSE: 0.0168\n",
      "Epoch [29100/100000], Loss: 0.0165, MSE: 0.0165\n",
      "Epoch [29200/100000], Loss: 0.0162, MSE: 0.0162\n",
      "Epoch [29300/100000], Loss: 0.0160, MSE: 0.0160\n",
      "Epoch [29400/100000], Loss: 0.0157, MSE: 0.0157\n",
      "Epoch [29500/100000], Loss: 0.0155, MSE: 0.0155\n",
      "Epoch [29600/100000], Loss: 0.0153, MSE: 0.0153\n",
      "Epoch [29700/100000], Loss: 0.0150, MSE: 0.0150\n",
      "Epoch [29800/100000], Loss: 0.0148, MSE: 0.0148\n",
      "Epoch [29900/100000], Loss: 0.0146, MSE: 0.0146\n",
      "Epoch [30000/100000], Loss: 0.0144, MSE: 0.0144\n",
      "Epoch [30100/100000], Loss: 0.0142, MSE: 0.0142\n",
      "Epoch [30200/100000], Loss: 0.0140, MSE: 0.0140\n",
      "Epoch [30300/100000], Loss: 0.0138, MSE: 0.0138\n",
      "Epoch [30400/100000], Loss: 0.0136, MSE: 0.0136\n",
      "Epoch [30500/100000], Loss: 0.0134, MSE: 0.0134\n",
      "Epoch [30600/100000], Loss: 0.0132, MSE: 0.0132\n",
      "Epoch [30700/100000], Loss: 0.0130, MSE: 0.0130\n",
      "Epoch [30800/100000], Loss: 0.0128, MSE: 0.0128\n",
      "Epoch [30900/100000], Loss: 0.0127, MSE: 0.0127\n",
      "Epoch [31000/100000], Loss: 0.0125, MSE: 0.0125\n",
      "Epoch [31100/100000], Loss: 0.0123, MSE: 0.0123\n",
      "Epoch [31200/100000], Loss: 0.0122, MSE: 0.0122\n",
      "Epoch [31300/100000], Loss: 0.0120, MSE: 0.0120\n",
      "Epoch [31400/100000], Loss: 0.0119, MSE: 0.0119\n",
      "Epoch [31500/100000], Loss: 0.0117, MSE: 0.0117\n",
      "Epoch [31600/100000], Loss: 0.0116, MSE: 0.0116\n",
      "Epoch [31700/100000], Loss: 0.0115, MSE: 0.0115\n",
      "Epoch [31800/100000], Loss: 0.0113, MSE: 0.0113\n",
      "Epoch [31900/100000], Loss: 0.0112, MSE: 0.0112\n",
      "Epoch [32000/100000], Loss: 0.0110, MSE: 0.0110\n",
      "Epoch [32100/100000], Loss: 0.0109, MSE: 0.0109\n",
      "Epoch [32200/100000], Loss: 0.0108, MSE: 0.0108\n",
      "Epoch [32300/100000], Loss: 0.0106, MSE: 0.0106\n",
      "Epoch [32400/100000], Loss: 0.0105, MSE: 0.0105\n",
      "Epoch [32500/100000], Loss: 0.0104, MSE: 0.0104\n",
      "Epoch [32600/100000], Loss: 0.0103, MSE: 0.0103\n",
      "Epoch [32700/100000], Loss: 0.0101, MSE: 0.0101\n",
      "Epoch [32800/100000], Loss: 0.0100, MSE: 0.0100\n",
      "Epoch [32900/100000], Loss: 0.0099, MSE: 0.0099\n",
      "Epoch [33000/100000], Loss: 0.0098, MSE: 0.0098\n",
      "Epoch [33100/100000], Loss: 0.0097, MSE: 0.0097\n",
      "Epoch [33200/100000], Loss: 0.0095, MSE: 0.0095\n",
      "Epoch [33300/100000], Loss: 0.0094, MSE: 0.0094\n",
      "Epoch [33400/100000], Loss: 0.0093, MSE: 0.0093\n",
      "Epoch [33500/100000], Loss: 0.0092, MSE: 0.0092\n",
      "Epoch [33600/100000], Loss: 0.0091, MSE: 0.0091\n",
      "Epoch [33700/100000], Loss: 0.0090, MSE: 0.0090\n",
      "Epoch [33800/100000], Loss: 0.0089, MSE: 0.0089\n",
      "Epoch [33900/100000], Loss: 0.0088, MSE: 0.0088\n",
      "Epoch [34000/100000], Loss: 0.0087, MSE: 0.0087\n",
      "Epoch [34100/100000], Loss: 0.0086, MSE: 0.0086\n",
      "Epoch [34200/100000], Loss: 0.0085, MSE: 0.0085\n",
      "Epoch [34300/100000], Loss: 0.0084, MSE: 0.0084\n",
      "Epoch [34400/100000], Loss: 0.0083, MSE: 0.0083\n",
      "Epoch [34500/100000], Loss: 0.0082, MSE: 0.0082\n",
      "Epoch [34600/100000], Loss: 0.0081, MSE: 0.0081\n",
      "Epoch [34700/100000], Loss: 0.0080, MSE: 0.0080\n",
      "Epoch [34800/100000], Loss: 0.0079, MSE: 0.0079\n",
      "Epoch [34900/100000], Loss: 0.0078, MSE: 0.0078\n",
      "Epoch [35000/100000], Loss: 0.0078, MSE: 0.0078\n",
      "Epoch [35100/100000], Loss: 0.0077, MSE: 0.0077\n",
      "Epoch [35200/100000], Loss: 0.0076, MSE: 0.0076\n",
      "Epoch [35300/100000], Loss: 0.0075, MSE: 0.0075\n",
      "Epoch [35400/100000], Loss: 0.0074, MSE: 0.0074\n",
      "Epoch [35500/100000], Loss: 0.0074, MSE: 0.0074\n",
      "Epoch [35600/100000], Loss: 0.0073, MSE: 0.0073\n",
      "Epoch [35700/100000], Loss: 0.0072, MSE: 0.0072\n",
      "Epoch [35800/100000], Loss: 0.0071, MSE: 0.0071\n",
      "Epoch [35900/100000], Loss: 0.0071, MSE: 0.0071\n",
      "Epoch [36000/100000], Loss: 0.0070, MSE: 0.0070\n",
      "Epoch [36100/100000], Loss: 0.0069, MSE: 0.0069\n",
      "Epoch [36200/100000], Loss: 0.0069, MSE: 0.0069\n",
      "Epoch [36300/100000], Loss: 0.0068, MSE: 0.0068\n",
      "Epoch [36400/100000], Loss: 0.0067, MSE: 0.0067\n",
      "Epoch [36500/100000], Loss: 0.0067, MSE: 0.0067\n",
      "Epoch [36600/100000], Loss: 0.0066, MSE: 0.0066\n",
      "Epoch [36700/100000], Loss: 0.0065, MSE: 0.0065\n",
      "Epoch [36800/100000], Loss: 0.0065, MSE: 0.0065\n",
      "Epoch [36900/100000], Loss: 0.0064, MSE: 0.0064\n",
      "Epoch [37000/100000], Loss: 0.0063, MSE: 0.0063\n",
      "Epoch [37100/100000], Loss: 0.0063, MSE: 0.0063\n",
      "Epoch [37200/100000], Loss: 0.0062, MSE: 0.0062\n",
      "Epoch [37300/100000], Loss: 0.0062, MSE: 0.0062\n",
      "Epoch [37400/100000], Loss: 0.0061, MSE: 0.0061\n",
      "Epoch [37500/100000], Loss: 0.0061, MSE: 0.0061\n",
      "Epoch [37600/100000], Loss: 0.0060, MSE: 0.0060\n",
      "Epoch [37700/100000], Loss: 0.0059, MSE: 0.0059\n",
      "Epoch [37800/100000], Loss: 0.0059, MSE: 0.0059\n",
      "Epoch [37900/100000], Loss: 0.0058, MSE: 0.0058\n",
      "Epoch [38000/100000], Loss: 0.0058, MSE: 0.0058\n",
      "Epoch [38100/100000], Loss: 0.0057, MSE: 0.0057\n",
      "Epoch [38200/100000], Loss: 0.0057, MSE: 0.0057\n",
      "Epoch [38300/100000], Loss: 0.0056, MSE: 0.0056\n",
      "Epoch [38400/100000], Loss: 0.0056, MSE: 0.0056\n",
      "Epoch [38500/100000], Loss: 0.0055, MSE: 0.0055\n",
      "Epoch [38600/100000], Loss: 0.0055, MSE: 0.0055\n",
      "Epoch [38700/100000], Loss: 0.0054, MSE: 0.0054\n",
      "Epoch [38800/100000], Loss: 0.0054, MSE: 0.0054\n",
      "Epoch [38900/100000], Loss: 0.0053, MSE: 0.0053\n",
      "Epoch [39000/100000], Loss: 0.0053, MSE: 0.0053\n",
      "Epoch [39100/100000], Loss: 0.0052, MSE: 0.0052\n",
      "Epoch [39200/100000], Loss: 0.0052, MSE: 0.0052\n",
      "Epoch [39300/100000], Loss: 0.0051, MSE: 0.0051\n",
      "Epoch [39400/100000], Loss: 0.0051, MSE: 0.0051\n",
      "Epoch [39500/100000], Loss: 0.0051, MSE: 0.0051\n",
      "Epoch [39600/100000], Loss: 0.0050, MSE: 0.0050\n",
      "Epoch [39700/100000], Loss: 0.0050, MSE: 0.0050\n",
      "Epoch [39800/100000], Loss: 0.0049, MSE: 0.0049\n",
      "Epoch [39900/100000], Loss: 0.0049, MSE: 0.0049\n",
      "Epoch [40000/100000], Loss: 0.0048, MSE: 0.0048\n",
      "Epoch [40100/100000], Loss: 0.0048, MSE: 0.0048\n",
      "Epoch [40200/100000], Loss: 0.0048, MSE: 0.0048\n",
      "Epoch [40300/100000], Loss: 0.0047, MSE: 0.0047\n",
      "Epoch [40400/100000], Loss: 0.0047, MSE: 0.0047\n",
      "Epoch [40500/100000], Loss: 0.0046, MSE: 0.0046\n",
      "Epoch [40600/100000], Loss: 0.0046, MSE: 0.0046\n",
      "Epoch [40700/100000], Loss: 0.0045, MSE: 0.0045\n",
      "Epoch [40800/100000], Loss: 0.0045, MSE: 0.0045\n",
      "Epoch [40900/100000], Loss: 0.0045, MSE: 0.0045\n",
      "Epoch [41000/100000], Loss: 0.0044, MSE: 0.0044\n",
      "Epoch [41100/100000], Loss: 0.0044, MSE: 0.0044\n",
      "Epoch [41200/100000], Loss: 0.0043, MSE: 0.0043\n",
      "Epoch [41300/100000], Loss: 0.0043, MSE: 0.0043\n",
      "Epoch [41400/100000], Loss: 0.0043, MSE: 0.0043\n",
      "Epoch [41500/100000], Loss: 0.0042, MSE: 0.0042\n",
      "Epoch [41600/100000], Loss: 0.0042, MSE: 0.0042\n",
      "Epoch [41700/100000], Loss: 0.0042, MSE: 0.0042\n",
      "Epoch [41800/100000], Loss: 0.0041, MSE: 0.0041\n",
      "Epoch [41900/100000], Loss: 0.0041, MSE: 0.0041\n",
      "Epoch [42000/100000], Loss: 0.0040, MSE: 0.0040\n",
      "Epoch [42100/100000], Loss: 0.0040, MSE: 0.0040\n",
      "Epoch [42200/100000], Loss: 0.0040, MSE: 0.0040\n",
      "Epoch [42300/100000], Loss: 0.0039, MSE: 0.0039\n",
      "Epoch [42400/100000], Loss: 0.0039, MSE: 0.0039\n",
      "Epoch [42500/100000], Loss: 0.0039, MSE: 0.0039\n",
      "Epoch [42600/100000], Loss: 0.0038, MSE: 0.0038\n",
      "Epoch [42700/100000], Loss: 0.0038, MSE: 0.0038\n",
      "Epoch [42800/100000], Loss: 0.0037, MSE: 0.0037\n",
      "Epoch [42900/100000], Loss: 0.0037, MSE: 0.0037\n",
      "Epoch [43000/100000], Loss: 0.0037, MSE: 0.0037\n",
      "Epoch [43100/100000], Loss: 0.0036, MSE: 0.0036\n",
      "Epoch [43200/100000], Loss: 0.0036, MSE: 0.0036\n",
      "Epoch [43300/100000], Loss: 0.0035, MSE: 0.0035\n",
      "Epoch [43400/100000], Loss: 0.0035, MSE: 0.0035\n",
      "Epoch [43500/100000], Loss: 0.0035, MSE: 0.0035\n",
      "Epoch [43600/100000], Loss: 0.0034, MSE: 0.0034\n",
      "Epoch [43700/100000], Loss: 0.0034, MSE: 0.0034\n",
      "Epoch [43800/100000], Loss: 0.0034, MSE: 0.0034\n",
      "Epoch [43900/100000], Loss: 0.0033, MSE: 0.0033\n",
      "Epoch [44000/100000], Loss: 0.0033, MSE: 0.0033\n",
      "Epoch [44100/100000], Loss: 0.0033, MSE: 0.0033\n",
      "Epoch [44200/100000], Loss: 0.0032, MSE: 0.0032\n",
      "Epoch [44300/100000], Loss: 0.0032, MSE: 0.0032\n",
      "Epoch [44400/100000], Loss: 0.0032, MSE: 0.0032\n",
      "Epoch [44500/100000], Loss: 0.0031, MSE: 0.0031\n",
      "Epoch [44600/100000], Loss: 0.0031, MSE: 0.0031\n",
      "Epoch [44700/100000], Loss: 0.0031, MSE: 0.0031\n",
      "Epoch [44800/100000], Loss: 0.0031, MSE: 0.0031\n",
      "Epoch [44900/100000], Loss: 0.0030, MSE: 0.0030\n",
      "Epoch [45000/100000], Loss: 0.0030, MSE: 0.0030\n",
      "Epoch [45100/100000], Loss: 0.0030, MSE: 0.0030\n",
      "Epoch [45200/100000], Loss: 0.0029, MSE: 0.0029\n",
      "Epoch [45300/100000], Loss: 0.0029, MSE: 0.0029\n",
      "Epoch [45400/100000], Loss: 0.0029, MSE: 0.0029\n",
      "Epoch [45500/100000], Loss: 0.0029, MSE: 0.0029\n",
      "Epoch [45600/100000], Loss: 0.0028, MSE: 0.0028\n",
      "Epoch [45700/100000], Loss: 0.0028, MSE: 0.0028\n",
      "Epoch [45800/100000], Loss: 0.0028, MSE: 0.0028\n",
      "Epoch [45900/100000], Loss: 0.0028, MSE: 0.0028\n",
      "Epoch [46000/100000], Loss: 0.0027, MSE: 0.0027\n",
      "Epoch [46100/100000], Loss: 0.0027, MSE: 0.0027\n",
      "Epoch [46200/100000], Loss: 0.0027, MSE: 0.0027\n",
      "Epoch [46300/100000], Loss: 0.0027, MSE: 0.0027\n",
      "Epoch [46400/100000], Loss: 0.0026, MSE: 0.0026\n",
      "Epoch [46500/100000], Loss: 0.0026, MSE: 0.0026\n",
      "Epoch [46600/100000], Loss: 0.0026, MSE: 0.0026\n",
      "Epoch [46700/100000], Loss: 0.0026, MSE: 0.0026\n",
      "Epoch [46800/100000], Loss: 0.0025, MSE: 0.0025\n",
      "Epoch [46900/100000], Loss: 0.0025, MSE: 0.0025\n",
      "Epoch [47000/100000], Loss: 0.0025, MSE: 0.0025\n",
      "Epoch [47100/100000], Loss: 0.0025, MSE: 0.0025\n",
      "Epoch [47200/100000], Loss: 0.0025, MSE: 0.0025\n",
      "Epoch [47300/100000], Loss: 0.0024, MSE: 0.0024\n",
      "Epoch [47400/100000], Loss: 0.0024, MSE: 0.0024\n",
      "Epoch [47500/100000], Loss: 0.0024, MSE: 0.0024\n",
      "Epoch [47600/100000], Loss: 0.0024, MSE: 0.0024\n",
      "Epoch [47700/100000], Loss: 0.0024, MSE: 0.0024\n",
      "Epoch [47800/100000], Loss: 0.0023, MSE: 0.0023\n",
      "Epoch [47900/100000], Loss: 0.0023, MSE: 0.0023\n",
      "Epoch [48000/100000], Loss: 0.0023, MSE: 0.0023\n",
      "Epoch [48100/100000], Loss: 0.0023, MSE: 0.0023\n",
      "Epoch [48200/100000], Loss: 0.0023, MSE: 0.0023\n",
      "Epoch [48300/100000], Loss: 0.0022, MSE: 0.0022\n",
      "Epoch [48400/100000], Loss: 0.0022, MSE: 0.0022\n",
      "Epoch [48500/100000], Loss: 0.0022, MSE: 0.0022\n",
      "Epoch [48600/100000], Loss: 0.0022, MSE: 0.0022\n",
      "Epoch [48700/100000], Loss: 0.0022, MSE: 0.0022\n",
      "Epoch [48800/100000], Loss: 0.0022, MSE: 0.0022\n",
      "Epoch [48900/100000], Loss: 0.0021, MSE: 0.0021\n",
      "Epoch [49000/100000], Loss: 0.0021, MSE: 0.0021\n",
      "Epoch [49100/100000], Loss: 0.0021, MSE: 0.0021\n",
      "Epoch [49200/100000], Loss: 0.0021, MSE: 0.0021\n",
      "Epoch [49300/100000], Loss: 0.0021, MSE: 0.0021\n",
      "Epoch [49400/100000], Loss: 0.0021, MSE: 0.0021\n",
      "Epoch [49500/100000], Loss: 0.0020, MSE: 0.0020\n",
      "Epoch [49600/100000], Loss: 0.0020, MSE: 0.0020\n",
      "Epoch [49700/100000], Loss: 0.0020, MSE: 0.0020\n",
      "Epoch [49800/100000], Loss: 0.0020, MSE: 0.0020\n",
      "Epoch [49900/100000], Loss: 0.0020, MSE: 0.0020\n",
      "Epoch [50000/100000], Loss: 0.0020, MSE: 0.0020\n",
      "Epoch [50100/100000], Loss: 0.0019, MSE: 0.0019\n",
      "Epoch [50200/100000], Loss: 0.0019, MSE: 0.0019\n",
      "Epoch [50300/100000], Loss: 0.0019, MSE: 0.0019\n",
      "Epoch [50400/100000], Loss: 0.0019, MSE: 0.0019\n",
      "Epoch [50500/100000], Loss: 0.0019, MSE: 0.0019\n",
      "Epoch [50600/100000], Loss: 0.0019, MSE: 0.0019\n",
      "Epoch [50700/100000], Loss: 0.0018, MSE: 0.0018\n",
      "Epoch [50800/100000], Loss: 0.0018, MSE: 0.0018\n",
      "Epoch [50900/100000], Loss: 0.0018, MSE: 0.0018\n",
      "Epoch [51000/100000], Loss: 0.0018, MSE: 0.0018\n",
      "Epoch [51100/100000], Loss: 0.0018, MSE: 0.0018\n",
      "Epoch [51200/100000], Loss: 0.0018, MSE: 0.0018\n",
      "Epoch [51300/100000], Loss: 0.0018, MSE: 0.0018\n",
      "Epoch [51400/100000], Loss: 0.0017, MSE: 0.0017\n",
      "Epoch [51500/100000], Loss: 0.0017, MSE: 0.0017\n",
      "Epoch [51600/100000], Loss: 0.0017, MSE: 0.0017\n",
      "Epoch [51700/100000], Loss: 0.0017, MSE: 0.0017\n",
      "Epoch [51800/100000], Loss: 0.0017, MSE: 0.0017\n",
      "Epoch [51900/100000], Loss: 0.0017, MSE: 0.0017\n",
      "Epoch [52000/100000], Loss: 0.0017, MSE: 0.0017\n",
      "Epoch [52100/100000], Loss: 0.0016, MSE: 0.0016\n",
      "Epoch [52200/100000], Loss: 0.0016, MSE: 0.0016\n",
      "Epoch [52300/100000], Loss: 0.0016, MSE: 0.0016\n",
      "Epoch [52400/100000], Loss: 0.0016, MSE: 0.0016\n",
      "Epoch [52500/100000], Loss: 0.0016, MSE: 0.0016\n",
      "Epoch [52600/100000], Loss: 0.0016, MSE: 0.0016\n",
      "Epoch [52700/100000], Loss: 0.0016, MSE: 0.0016\n",
      "Epoch [52800/100000], Loss: 0.0015, MSE: 0.0015\n",
      "Epoch [52900/100000], Loss: 0.0015, MSE: 0.0015\n",
      "Epoch [53000/100000], Loss: 0.0015, MSE: 0.0015\n",
      "Epoch [53100/100000], Loss: 0.0015, MSE: 0.0015\n",
      "Epoch [53200/100000], Loss: 0.0015, MSE: 0.0015\n",
      "Epoch [53300/100000], Loss: 0.0015, MSE: 0.0015\n",
      "Epoch [53400/100000], Loss: 0.0015, MSE: 0.0015\n",
      "Epoch [53500/100000], Loss: 0.0015, MSE: 0.0015\n",
      "Epoch [53600/100000], Loss: 0.0014, MSE: 0.0014\n",
      "Epoch [53700/100000], Loss: 0.0014, MSE: 0.0014\n",
      "Epoch [53800/100000], Loss: 0.0014, MSE: 0.0014\n",
      "Epoch [53900/100000], Loss: 0.0014, MSE: 0.0014\n",
      "Epoch [54000/100000], Loss: 0.0014, MSE: 0.0014\n",
      "Epoch [54100/100000], Loss: 0.0014, MSE: 0.0014\n",
      "Epoch [54200/100000], Loss: 0.0014, MSE: 0.0014\n",
      "Epoch [54300/100000], Loss: 0.0014, MSE: 0.0014\n",
      "Epoch [54400/100000], Loss: 0.0013, MSE: 0.0013\n",
      "Epoch [54500/100000], Loss: 0.0013, MSE: 0.0013\n",
      "Epoch [54600/100000], Loss: 0.0013, MSE: 0.0013\n",
      "Epoch [54700/100000], Loss: 0.0013, MSE: 0.0013\n",
      "Epoch [54800/100000], Loss: 0.0013, MSE: 0.0013\n",
      "Epoch [54900/100000], Loss: 0.0013, MSE: 0.0013\n",
      "Epoch [55000/100000], Loss: 0.0013, MSE: 0.0013\n",
      "Epoch [55100/100000], Loss: 0.0013, MSE: 0.0013\n",
      "Epoch [55200/100000], Loss: 0.0013, MSE: 0.0013\n",
      "Epoch [55300/100000], Loss: 0.0012, MSE: 0.0012\n",
      "Epoch [55400/100000], Loss: 0.0012, MSE: 0.0012\n",
      "Epoch [55500/100000], Loss: 0.0012, MSE: 0.0012\n",
      "Epoch [55600/100000], Loss: 0.0012, MSE: 0.0012\n",
      "Epoch [55700/100000], Loss: 0.0012, MSE: 0.0012\n",
      "Epoch [55800/100000], Loss: 0.0012, MSE: 0.0012\n",
      "Epoch [55900/100000], Loss: 0.0012, MSE: 0.0012\n",
      "Epoch [56000/100000], Loss: 0.0012, MSE: 0.0012\n",
      "Epoch [56100/100000], Loss: 0.0012, MSE: 0.0012\n",
      "Epoch [56200/100000], Loss: 0.0011, MSE: 0.0011\n",
      "Epoch [56300/100000], Loss: 0.0011, MSE: 0.0011\n",
      "Epoch [56400/100000], Loss: 0.0011, MSE: 0.0011\n",
      "Epoch [56500/100000], Loss: 0.0011, MSE: 0.0011\n",
      "Epoch [56600/100000], Loss: 0.0011, MSE: 0.0011\n",
      "Epoch [56700/100000], Loss: 0.0011, MSE: 0.0011\n",
      "Epoch [56800/100000], Loss: 0.0011, MSE: 0.0011\n",
      "Epoch [56900/100000], Loss: 0.0011, MSE: 0.0011\n",
      "Epoch [57000/100000], Loss: 0.0011, MSE: 0.0011\n",
      "Epoch [57100/100000], Loss: 0.0011, MSE: 0.0011\n",
      "Epoch [57200/100000], Loss: 0.0010, MSE: 0.0010\n",
      "Epoch [57300/100000], Loss: 0.0010, MSE: 0.0010\n",
      "Epoch [57400/100000], Loss: 0.0010, MSE: 0.0010\n",
      "Epoch [57500/100000], Loss: 0.0010, MSE: 0.0010\n",
      "Epoch [57600/100000], Loss: 0.0010, MSE: 0.0010\n",
      "Epoch [57700/100000], Loss: 0.0010, MSE: 0.0010\n",
      "Epoch [57800/100000], Loss: 0.0010, MSE: 0.0010\n",
      "Epoch [57900/100000], Loss: 0.0010, MSE: 0.0010\n",
      "Epoch [58000/100000], Loss: 0.0010, MSE: 0.0010\n",
      "Epoch [58100/100000], Loss: 0.0010, MSE: 0.0010\n",
      "Epoch [58200/100000], Loss: 0.0010, MSE: 0.0010\n",
      "Epoch [58300/100000], Loss: 0.0010, MSE: 0.0010\n",
      "Epoch [58400/100000], Loss: 0.0009, MSE: 0.0009\n",
      "Epoch [58500/100000], Loss: 0.0009, MSE: 0.0009\n",
      "Epoch [58600/100000], Loss: 0.0009, MSE: 0.0009\n",
      "Epoch [58700/100000], Loss: 0.0009, MSE: 0.0009\n",
      "Epoch [58800/100000], Loss: 0.0009, MSE: 0.0009\n",
      "Epoch [58900/100000], Loss: 0.0009, MSE: 0.0009\n",
      "Epoch [59000/100000], Loss: 0.0009, MSE: 0.0009\n",
      "Epoch [59100/100000], Loss: 0.0009, MSE: 0.0009\n",
      "Epoch [59200/100000], Loss: 0.0009, MSE: 0.0009\n",
      "Epoch [59300/100000], Loss: 0.0009, MSE: 0.0009\n",
      "Epoch [59400/100000], Loss: 0.0009, MSE: 0.0009\n",
      "Epoch [59500/100000], Loss: 0.0009, MSE: 0.0009\n",
      "Epoch [59600/100000], Loss: 0.0008, MSE: 0.0008\n",
      "Epoch [59700/100000], Loss: 0.0008, MSE: 0.0008\n",
      "Epoch [59800/100000], Loss: 0.0008, MSE: 0.0008\n",
      "Epoch [59900/100000], Loss: 0.0008, MSE: 0.0008\n",
      "Epoch [60000/100000], Loss: 0.0008, MSE: 0.0008\n",
      "Epoch [60100/100000], Loss: 0.0008, MSE: 0.0008\n",
      "Epoch [60200/100000], Loss: 0.0008, MSE: 0.0008\n",
      "Epoch [60300/100000], Loss: 0.0008, MSE: 0.0008\n",
      "Epoch [60400/100000], Loss: 0.0008, MSE: 0.0008\n",
      "Epoch [60500/100000], Loss: 0.0008, MSE: 0.0008\n",
      "Epoch [60600/100000], Loss: 0.0008, MSE: 0.0008\n",
      "Epoch [60700/100000], Loss: 0.0008, MSE: 0.0008\n",
      "Epoch [60800/100000], Loss: 0.0008, MSE: 0.0008\n",
      "Epoch [60900/100000], Loss: 0.0008, MSE: 0.0008\n",
      "Epoch [61000/100000], Loss: 0.0007, MSE: 0.0007\n",
      "Epoch [61100/100000], Loss: 0.0007, MSE: 0.0007\n",
      "Epoch [61200/100000], Loss: 0.0007, MSE: 0.0007\n",
      "Epoch [61300/100000], Loss: 0.0007, MSE: 0.0007\n",
      "Epoch [61400/100000], Loss: 0.0007, MSE: 0.0007\n",
      "Epoch [61500/100000], Loss: 0.0007, MSE: 0.0007\n",
      "Epoch [61600/100000], Loss: 0.0007, MSE: 0.0007\n",
      "Epoch [61700/100000], Loss: 0.0007, MSE: 0.0007\n",
      "Epoch [61800/100000], Loss: 0.0007, MSE: 0.0007\n",
      "Epoch [61900/100000], Loss: 0.0007, MSE: 0.0007\n",
      "Epoch [62000/100000], Loss: 0.0007, MSE: 0.0007\n",
      "Epoch [62100/100000], Loss: 0.0007, MSE: 0.0007\n",
      "Epoch [62200/100000], Loss: 0.0007, MSE: 0.0007\n",
      "Epoch [62300/100000], Loss: 0.0007, MSE: 0.0007\n",
      "Epoch [62400/100000], Loss: 0.0007, MSE: 0.0007\n",
      "Epoch [62500/100000], Loss: 0.0007, MSE: 0.0007\n",
      "Epoch [62600/100000], Loss: 0.0006, MSE: 0.0006\n",
      "Epoch [62700/100000], Loss: 0.0006, MSE: 0.0006\n",
      "Epoch [62800/100000], Loss: 0.0006, MSE: 0.0006\n",
      "Epoch [62900/100000], Loss: 0.0006, MSE: 0.0006\n",
      "Epoch [63000/100000], Loss: 0.0006, MSE: 0.0006\n",
      "Epoch [63100/100000], Loss: 0.0006, MSE: 0.0006\n",
      "Epoch [63200/100000], Loss: 0.0006, MSE: 0.0006\n",
      "Epoch [63300/100000], Loss: 0.0006, MSE: 0.0006\n",
      "Epoch [63400/100000], Loss: 0.0006, MSE: 0.0006\n",
      "Epoch [63500/100000], Loss: 0.0006, MSE: 0.0006\n",
      "Epoch [63600/100000], Loss: 0.0006, MSE: 0.0006\n",
      "Epoch [63700/100000], Loss: 0.0006, MSE: 0.0006\n",
      "Epoch [63800/100000], Loss: 0.0006, MSE: 0.0006\n",
      "Epoch [63900/100000], Loss: 0.0006, MSE: 0.0006\n",
      "Epoch [64000/100000], Loss: 0.0006, MSE: 0.0006\n",
      "Epoch [64100/100000], Loss: 0.0006, MSE: 0.0006\n",
      "Epoch [64200/100000], Loss: 0.0006, MSE: 0.0006\n",
      "Epoch [64300/100000], Loss: 0.0006, MSE: 0.0006\n",
      "Epoch [64400/100000], Loss: 0.0005, MSE: 0.0005\n",
      "Epoch [64500/100000], Loss: 0.0005, MSE: 0.0005\n",
      "Epoch [64600/100000], Loss: 0.0005, MSE: 0.0005\n",
      "Epoch [64700/100000], Loss: 0.0005, MSE: 0.0005\n",
      "Epoch [64800/100000], Loss: 0.0005, MSE: 0.0005\n",
      "Epoch [64900/100000], Loss: 0.0005, MSE: 0.0005\n",
      "Epoch [65000/100000], Loss: 0.0005, MSE: 0.0005\n",
      "Epoch [65100/100000], Loss: 0.0005, MSE: 0.0005\n",
      "Epoch [65200/100000], Loss: 0.0005, MSE: 0.0005\n",
      "Epoch [65300/100000], Loss: 0.0005, MSE: 0.0005\n",
      "Epoch [65400/100000], Loss: 0.0005, MSE: 0.0005\n",
      "Epoch [65500/100000], Loss: 0.0005, MSE: 0.0005\n",
      "Epoch [65600/100000], Loss: 0.0005, MSE: 0.0005\n",
      "Epoch [65700/100000], Loss: 0.0005, MSE: 0.0005\n",
      "Epoch [65800/100000], Loss: 0.0005, MSE: 0.0005\n",
      "Epoch [65900/100000], Loss: 0.0005, MSE: 0.0005\n",
      "Epoch [66000/100000], Loss: 0.0005, MSE: 0.0005\n",
      "Epoch [66100/100000], Loss: 0.0005, MSE: 0.0005\n",
      "Epoch [66200/100000], Loss: 0.0005, MSE: 0.0005\n",
      "Epoch [66300/100000], Loss: 0.0005, MSE: 0.0005\n",
      "Epoch [66400/100000], Loss: 0.0004, MSE: 0.0004\n",
      "Epoch [66500/100000], Loss: 0.0004, MSE: 0.0004\n",
      "Epoch [66600/100000], Loss: 0.0004, MSE: 0.0004\n",
      "Epoch [66700/100000], Loss: 0.0004, MSE: 0.0004\n",
      "Epoch [66800/100000], Loss: 0.0004, MSE: 0.0004\n",
      "Epoch [66900/100000], Loss: 0.0004, MSE: 0.0004\n",
      "Epoch [67000/100000], Loss: 0.0004, MSE: 0.0004\n",
      "Epoch [67100/100000], Loss: 0.0004, MSE: 0.0004\n",
      "Epoch [67200/100000], Loss: 0.0004, MSE: 0.0004\n",
      "Epoch [67300/100000], Loss: 0.0004, MSE: 0.0004\n",
      "Epoch [67400/100000], Loss: 0.0004, MSE: 0.0004\n",
      "Epoch [67500/100000], Loss: 0.0004, MSE: 0.0004\n",
      "Epoch [67600/100000], Loss: 0.0004, MSE: 0.0004\n",
      "Epoch [67700/100000], Loss: 0.0004, MSE: 0.0004\n",
      "Epoch [67800/100000], Loss: 0.0004, MSE: 0.0004\n",
      "Epoch [67900/100000], Loss: 0.0004, MSE: 0.0004\n",
      "Epoch [68000/100000], Loss: 0.0004, MSE: 0.0004\n",
      "Epoch [68100/100000], Loss: 0.0004, MSE: 0.0004\n",
      "Epoch [68200/100000], Loss: 0.0004, MSE: 0.0004\n",
      "Epoch [68300/100000], Loss: 0.0004, MSE: 0.0004\n",
      "Epoch [68400/100000], Loss: 0.0004, MSE: 0.0004\n",
      "Epoch [68500/100000], Loss: 0.0004, MSE: 0.0004\n",
      "Epoch [68600/100000], Loss: 0.0004, MSE: 0.0004\n",
      "Epoch [68700/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [68800/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [68900/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [69000/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [69100/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [69200/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [69300/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [69400/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [69500/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [69600/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [69700/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [69800/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [69900/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [70000/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [70100/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [70200/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [70300/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [70400/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [70500/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [70600/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [70700/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [70800/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [70900/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [71000/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [71100/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [71200/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [71300/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [71400/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [71500/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [71600/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [71700/100000], Loss: 0.0003, MSE: 0.0003\n",
      "Epoch [71800/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [71900/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [72000/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [72100/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [72200/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [72300/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [72400/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [72500/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [72600/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [72700/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [72800/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [72900/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [73000/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [73100/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [73200/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [73300/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [73400/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [73500/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [73600/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [73700/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [73800/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [73900/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [74000/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [74100/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [74200/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [74300/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [74400/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [74500/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [74600/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [74700/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [74800/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [74900/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [75000/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [75100/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [75200/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [75300/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [75400/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [75500/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [75600/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [75700/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [75800/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [75900/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [76000/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [76100/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [76200/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [76300/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [76400/100000], Loss: 0.0002, MSE: 0.0002\n",
      "Epoch [76500/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [76600/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [76700/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [76800/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [76900/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [77000/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [77100/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [77200/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [77300/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [77400/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [77500/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [77600/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [77700/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [77800/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [77900/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [78000/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [78100/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [78200/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [78300/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [78400/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [78500/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [78600/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [78700/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [78800/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [78900/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [79000/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [79100/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [79200/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [79300/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [79400/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [79500/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [79600/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [79700/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [79800/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [79900/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [80000/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [80100/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [80200/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [80300/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [80400/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [80500/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [80600/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [80700/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [80800/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [80900/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [81000/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [81100/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [81200/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [81300/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [81400/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [81500/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [81600/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [81700/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [81800/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [81900/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [82000/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [82100/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [82200/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [82300/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [82400/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [82500/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [82600/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [82700/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [82800/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [82900/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [83000/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [83100/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [83200/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [83300/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [83400/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [83500/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [83600/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [83700/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [83800/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [83900/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [84000/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [84100/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [84200/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [84300/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [84400/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [84500/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [84600/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [84700/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [84800/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [84900/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [85000/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [85100/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [85200/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [85300/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [85400/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [85500/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [85600/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [85700/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [85800/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [85900/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [86000/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [86100/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [86200/100000], Loss: 0.0001, MSE: 0.0001\n",
      "Epoch [86300/100000], Loss: 0.0000, MSE: 0.0000\n",
      "Epoch [86400/100000], Loss: 0.0000, MSE: 0.0000\n",
      "Epoch [86500/100000], Loss: 0.0000, MSE: 0.0000\n",
      "Epoch [86600/100000], Loss: 0.0000, MSE: 0.0000\n",
      "Epoch [86700/100000], Loss: 0.0000, MSE: 0.0000\n",
      "Epoch [86800/100000], Loss: 0.0000, MSE: 0.0000\n",
      "Epoch [86900/100000], Loss: 0.0000, MSE: 0.0000\n",
      "Epoch [87000/100000], Loss: 0.0000, MSE: 0.0000\n",
      "Epoch [87100/100000], Loss: 0.0000, MSE: 0.0000\n",
      "Epoch [87200/100000], Loss: 0.0000, MSE: 0.0000\n",
      "Epoch [87300/100000], Loss: 0.0000, MSE: 0.0000\n",
      "Epoch [87400/100000], Loss: 0.0000, MSE: 0.0000\n",
      "Epoch [87500/100000], Loss: 0.0000, MSE: 0.0000\n",
      "Epoch [87600/100000], Loss: 0.0000, MSE: 0.0000\n",
      "Epoch [87700/100000], Loss: 0.0000, MSE: 0.0000\n",
      "Epoch [87800/100000], Loss: 0.0000, MSE: 0.0000\n",
      "Epoch [87900/100000], Loss: 0.0000, MSE: 0.0000\n",
      "Epoch [88000/100000], Loss: 0.0000, MSE: 0.0000\n",
      "Epoch [88100/100000], Loss: 0.0000, MSE: 0.0000\n",
      "Epoch [88200/100000], Loss: 0.0000, MSE: 0.0000\n",
      "Epoch [88300/100000], Loss: 0.0000, MSE: 0.0000\n",
      "Epoch [88400/100000], Loss: 0.0000, MSE: 0.0000\n",
      "Epoch [88500/100000], Loss: 0.0000, MSE: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      5\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 6\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_train)\n\u001b[0;32m      8\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32md:\\GitHub_Repo\\torchstudy\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\GitHub_Repo\\torchstudy\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[85], line 12\u001b[0m, in \u001b[0;36mRegressionModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x))\n",
      "File \u001b[1;32md:\\GitHub_Repo\\torchstudy\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\GitHub_Repo\\torchstudy\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\GitHub_Repo\\torchstudy\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "num_epochs = 100000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(x_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Calculate mean squared error\n",
    "    mse_value = mse(outputs, y_train)\n",
    "    \n",
    "    # Log statistics to TensorBoard\n",
    "    writer.add_scalar('Loss/train', loss.item(), epoch)\n",
    "    writer.add_scalar('MSE/train', mse_value.item(), epoch)\n",
    "    writer.add_scalar('Accuracy/train', 1 - mse_value.item(), epoch)\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, MSE: {mse_value.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "close-writer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[-0.0944,  0.1167,  0.1563,  0.0251,  0.0895, -0.1014,  0.0483, -0.1141,\n",
       "                       -0.1614, -0.2342, -0.1861,  0.2656,  0.2835, -0.2363,  0.0083],\n",
       "                      [ 0.0584, -0.2006, -0.1323, -0.0662,  0.0887,  0.2577,  0.0392, -0.2535,\n",
       "                       -0.3521, -0.0745, -0.1424,  0.1626,  0.0130,  0.1010, -0.2513],\n",
       "                      [ 0.1836, -0.1205, -0.1757, -0.3431, -0.2406,  0.2391, -0.1814, -0.0614,\n",
       "                       -0.0343,  0.1429,  0.2187,  0.0414,  0.0057,  0.1165,  0.1349],\n",
       "                      [ 0.2158,  0.2239, -0.0666, -0.1701,  0.0715,  0.0126, -0.1541, -0.1334,\n",
       "                        0.0704, -0.2005, -0.2476,  0.0849,  0.1519, -0.0816,  0.2467],\n",
       "                      [ 0.0892,  0.0385,  0.1583,  0.2663, -0.1960,  0.2111, -0.2259,  0.0799,\n",
       "                       -0.0677, -0.0230, -0.0585,  0.1929,  0.0549,  0.3540, -0.1603],\n",
       "                      [-0.1449,  0.0829,  0.0587, -0.2563,  0.0342,  0.0166, -0.1537,  0.1412,\n",
       "                        0.0943, -0.1968,  0.1282,  0.1509, -0.1349,  0.0077,  0.2783],\n",
       "                      [ 0.2423,  0.0196,  0.0656, -0.1465,  0.1869,  0.0653, -0.0946, -0.0818,\n",
       "                        0.0502,  0.1404, -0.0066, -0.1880,  0.1161,  0.0030, -0.1389],\n",
       "                      [ 0.0413, -0.1445,  0.0339,  0.3118,  0.1141, -0.3392,  0.1326, -0.3791,\n",
       "                       -0.0290,  0.1914,  0.1529,  0.2041,  0.2350, -0.1151,  0.1096],\n",
       "                      [ 0.0855,  0.1584,  0.0851, -0.1954,  0.1708, -0.0332,  0.2675,  0.0855,\n",
       "                       -0.2692,  0.1030, -0.3126,  0.0436,  0.0858, -0.3946,  0.3046],\n",
       "                      [-0.1088,  0.4816,  0.0342,  0.0858,  0.2649,  0.2093,  0.0400,  0.0014,\n",
       "                       -0.1109, -0.1404,  0.0997, -0.1962,  0.3283, -0.0376,  0.0016]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([ 0.0912, -0.1734, -0.0570,  0.2245, -0.0757, -0.2696,  0.1018, -0.1854,\n",
       "                      -0.0302, -0.1706])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[-0.1411, -0.2897, -0.3064,  0.1416, -0.0291, -0.0184, -0.2806,  0.2537,\n",
       "                        0.0126, -0.1101],\n",
       "                      [-0.2465,  0.1501, -0.0635,  0.2070,  0.2064,  0.0400,  0.2973, -0.2591,\n",
       "                       -0.2714, -0.3877],\n",
       "                      [ 0.1643, -0.2044,  0.1206, -0.0933, -0.2083, -0.1934, -0.0158, -0.2957,\n",
       "                        0.1060,  0.0486],\n",
       "                      [ 0.1519, -0.1623, -0.1041, -0.2989,  0.2673,  0.2274, -0.2869,  0.0824,\n",
       "                       -0.3015,  0.3318],\n",
       "                      [ 0.2958,  0.3537, -0.1781, -0.1484, -0.3979, -0.2163, -0.0644,  0.3874,\n",
       "                       -0.2693,  0.3270],\n",
       "                      [-0.0159,  0.1802,  0.1181,  0.1433,  0.1491,  0.3416, -0.1450, -0.2571,\n",
       "                        0.1763, -0.0196],\n",
       "                      [-0.0360,  0.2259,  0.1118, -0.1295,  0.1095,  0.3011, -0.1235,  0.0481,\n",
       "                       -0.2730, -0.0700],\n",
       "                      [ 0.1545, -0.0882, -0.0543,  0.2111,  0.1344,  0.1312,  0.0286,  0.1313,\n",
       "                        0.5252, -0.3924],\n",
       "                      [-0.1148, -0.0313, -0.1239, -0.0716, -0.2346, -0.0486, -0.3179,  0.3994,\n",
       "                        0.1003,  0.4412],\n",
       "                      [ 0.1465, -0.1393,  0.0080, -0.1901, -0.0442, -0.2791,  0.0543, -0.2286,\n",
       "                       -0.2647, -0.2830]])),\n",
       "             ('fc2.bias',\n",
       "              tensor([ 1.7335e-01,  1.1381e-01, -2.3079e-01, -1.9202e-01,  4.4306e-01,\n",
       "                       2.4812e-01,  2.0390e-01, -5.9421e-05,  3.3566e-01,  6.1822e-02])),\n",
       "             ('fc3.weight',\n",
       "              tensor([[-0.2479,  0.1447, -0.0101,  0.2343, -0.5010,  0.3711, -0.1168,  0.4552,\n",
       "                        0.1911, -0.0825],\n",
       "                      [-0.0311, -0.0315,  0.3122, -0.0741, -0.2690, -0.1916,  0.2544, -0.2752,\n",
       "                        0.2313, -0.2280],\n",
       "                      [-0.2423, -0.1530, -0.2432,  0.2423,  0.5851, -0.0359,  0.0379, -0.3790,\n",
       "                        0.5289,  0.0592],\n",
       "                      [-0.0718, -0.0102, -0.2309, -0.1135,  0.0506, -0.2106, -0.2663,  0.0638,\n",
       "                       -0.1609,  0.1042],\n",
       "                      [ 0.0542, -0.2464, -0.2138,  0.1252,  0.1115, -0.2008,  0.0062,  0.2746,\n",
       "                       -0.2337, -0.0814],\n",
       "                      [-0.2249,  0.2935,  0.1927, -0.1912, -0.0549, -0.0189, -0.2990, -0.2145,\n",
       "                        0.1962, -0.0509],\n",
       "                      [-0.2710,  0.2100, -0.2113, -0.2038,  0.0544, -0.2496, -0.2816,  0.4022,\n",
       "                        0.0343, -0.1627],\n",
       "                      [ 0.1945, -0.3487,  0.2993,  0.1022,  0.4332, -0.0664, -0.1906, -0.3502,\n",
       "                        0.4734,  0.2172],\n",
       "                      [ 0.1055, -0.0293, -0.1007, -0.2262,  0.2724, -0.0734, -0.0343, -0.1442,\n",
       "                       -0.1914,  0.1661],\n",
       "                      [-0.1918, -0.3015, -0.1373, -0.0898,  0.1457,  0.0964,  0.0955, -0.0235,\n",
       "                        0.0765,  0.1797]])),\n",
       "             ('fc3.bias',\n",
       "              tensor([ 0.2879,  0.2353,  0.1396,  0.0233, -0.2271, -0.1373,  0.1187,  0.1852,\n",
       "                       0.1337, -0.0215])),\n",
       "             ('output.weight',\n",
       "              tensor([[-0.6561,  0.0338,  0.7925, -0.0142, -0.2411, -0.0142, -0.3439,  0.6994,\n",
       "                        0.1215,  0.0578]])),\n",
       "             ('output.bias', tensor([-0.4172]))])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Close the TensorBoard writer\n",
    "writer.close()\n",
    "\n",
    "model.state_dict()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
