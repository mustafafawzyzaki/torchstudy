{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[515.7012, 490.2841, 510.3097,  ..., 490.2304, 500.2619, 506.8944],\n",
      "        [494.7331, 509.0645, 501.6894,  ..., 499.8628, 502.1999, 481.8784],\n",
      "        [505.5822, 500.7718, 503.6093,  ..., 493.6699, 505.8808, 507.2207],\n",
      "        ...,\n",
      "        [493.5047, 512.9279, 501.6550,  ..., 503.0477, 489.9249, 495.9157],\n",
      "        [492.7509, 505.6155, 509.0760,  ..., 500.8198, 500.3739, 498.5204],\n",
      "        [511.4201, 499.0053, 506.7788,  ..., 506.1491, 493.3253, 503.5687]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(1000000,10 , dtype=torch.float32 , requires_grad=True)\n",
    "Y = torch.randn(1000000,10 , dtype=torch.float32 , requires_grad=True)\n",
    "Z = 5* X + 7 *Y + 500\n",
    "\n",
    "print (Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR (nn.Module ):\n",
    "    def __init__(self):\n",
    "        super(LR , self).__init__()\n",
    "        self.weight1 = nn.Parameter(torch.randn(1, dtype=torch.float32) , requires_grad=True) \n",
    "        self.weight2 = nn.Parameter(torch.randn(1, dtype=torch.float32) , requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.randn(1, dtype=torch.float32) , requires_grad=True)\n",
    "        \n",
    "    def forward(self , X , Y) : \n",
    "        return  (self.weight1) * X + (self.weight2) * Y + (self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000000, 10), (1000000, 10))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "model1 = LR()\n",
    "model1.state_dict()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model1.parameters() , lr=0.001)\n",
    "\n",
    "model1.eval()    \n",
    "with torch.inference_mode():\n",
    "    predict = model1(X,Y)\n",
    "    \n",
    "    \n",
    "X_numpy = X.cpu().detach().numpy().tolist()\n",
    "Y_numpy = Y.cpu().detach().numpy().tolist()\n",
    "Z_numpy = Z.cpu().detach().numpy()\n",
    "predict_numpy = predict.cpu().detach().numpy()\n",
    "\n",
    "Z_numpy.shape , predict_numpy.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000000, 10]),\n",
       " torch.Size([1000000, 10]),\n",
       " torch.Size([1000000, 10]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = LR()\n",
    "\n",
    "\n",
    "X.shape , Y.shape , Z.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(model2.parameters() , lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model , X , Y , Z , loss_function , optimizer , epoches):\n",
    "    epoches_list = []\n",
    "    tr_losses = []\n",
    "    test_losses= []\n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        predict = model(X,Y)\n",
    "        tr_loss = loss_function(predict , Z)\n",
    "        tr_loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            test_predict = model(X,Y)\n",
    "            test_loss = loss_function(test_predict , Z)\n",
    "            \n",
    "        if epoch % 100 == 0 :\n",
    "            epoches_list.append(epoch)\n",
    "            tr_losses.append(tr_loss)\n",
    "            test_losses.append(test_loss)\n",
    "        print(f'Epoch {epoch} , Train Loss {tr_loss.item()} , Test Loss {test_loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 , Train Loss 471.7832336425781 , Test Loss 470.7831726074219\n",
      "Epoch 1 , Train Loss 470.7831726074219 , Test Loss 469.783203125\n",
      "Epoch 2 , Train Loss 469.783203125 , Test Loss 468.783203125\n",
      "Epoch 3 , Train Loss 468.783203125 , Test Loss 467.7832336425781\n",
      "Epoch 4 , Train Loss 467.7832336425781 , Test Loss 466.7831726074219\n",
      "Epoch 5 , Train Loss 466.7831726074219 , Test Loss 465.78314208984375\n",
      "Epoch 6 , Train Loss 465.78314208984375 , Test Loss 464.78314208984375\n",
      "Epoch 7 , Train Loss 464.78314208984375 , Test Loss 463.7832336425781\n",
      "Epoch 8 , Train Loss 463.7832336425781 , Test Loss 462.7831726074219\n",
      "Epoch 9 , Train Loss 462.7831726074219 , Test Loss 461.783203125\n",
      "Epoch 10 , Train Loss 461.783203125 , Test Loss 460.78314208984375\n",
      "Epoch 11 , Train Loss 460.78314208984375 , Test Loss 459.7832336425781\n",
      "Epoch 12 , Train Loss 459.7832336425781 , Test Loss 458.7832336425781\n",
      "Epoch 13 , Train Loss 458.7832336425781 , Test Loss 457.783203125\n",
      "Epoch 14 , Train Loss 457.783203125 , Test Loss 456.78314208984375\n",
      "Epoch 15 , Train Loss 456.78314208984375 , Test Loss 455.7832336425781\n",
      "Epoch 16 , Train Loss 455.7832336425781 , Test Loss 454.7831726074219\n",
      "Epoch 17 , Train Loss 454.7831726074219 , Test Loss 453.783203125\n",
      "Epoch 18 , Train Loss 453.783203125 , Test Loss 452.78314208984375\n",
      "Epoch 19 , Train Loss 452.78314208984375 , Test Loss 451.7832336425781\n",
      "Epoch 20 , Train Loss 451.7832336425781 , Test Loss 450.7832336425781\n",
      "Epoch 21 , Train Loss 450.7832336425781 , Test Loss 449.783203125\n",
      "Epoch 22 , Train Loss 449.783203125 , Test Loss 448.78314208984375\n",
      "Epoch 23 , Train Loss 448.78314208984375 , Test Loss 447.7832336425781\n",
      "Epoch 24 , Train Loss 447.7832336425781 , Test Loss 446.7832336425781\n",
      "Epoch 25 , Train Loss 446.7832336425781 , Test Loss 445.783203125\n",
      "Epoch 26 , Train Loss 445.783203125 , Test Loss 444.783203125\n",
      "Epoch 27 , Train Loss 444.783203125 , Test Loss 443.7832336425781\n",
      "Epoch 28 , Train Loss 443.7832336425781 , Test Loss 442.7832336425781\n",
      "Epoch 29 , Train Loss 442.7832336425781 , Test Loss 441.783203125\n",
      "Epoch 30 , Train Loss 441.783203125 , Test Loss 440.78314208984375\n",
      "Epoch 31 , Train Loss 440.78314208984375 , Test Loss 439.7832336425781\n",
      "Epoch 32 , Train Loss 439.7832336425781 , Test Loss 438.7832336425781\n",
      "Epoch 33 , Train Loss 438.7832336425781 , Test Loss 437.783203125\n",
      "Epoch 34 , Train Loss 437.783203125 , Test Loss 436.78314208984375\n",
      "Epoch 35 , Train Loss 436.78314208984375 , Test Loss 435.7832336425781\n",
      "Epoch 36 , Train Loss 435.7832336425781 , Test Loss 434.7832336425781\n",
      "Epoch 37 , Train Loss 434.7832336425781 , Test Loss 433.783203125\n",
      "Epoch 38 , Train Loss 433.783203125 , Test Loss 432.78314208984375\n",
      "Epoch 39 , Train Loss 432.78314208984375 , Test Loss 431.7832336425781\n",
      "Epoch 40 , Train Loss 431.7832336425781 , Test Loss 430.7832336425781\n",
      "Epoch 41 , Train Loss 430.7832336425781 , Test Loss 429.783203125\n",
      "Epoch 42 , Train Loss 429.783203125 , Test Loss 428.7831726074219\n",
      "Epoch 43 , Train Loss 428.7831726074219 , Test Loss 427.7832336425781\n",
      "Epoch 44 , Train Loss 427.7832336425781 , Test Loss 426.7831726074219\n",
      "Epoch 45 , Train Loss 426.7831726074219 , Test Loss 425.7832336425781\n",
      "Epoch 46 , Train Loss 425.7832336425781 , Test Loss 424.7831726074219\n",
      "Epoch 47 , Train Loss 424.7831726074219 , Test Loss 423.7832336425781\n",
      "Epoch 48 , Train Loss 423.7832336425781 , Test Loss 422.7832336425781\n",
      "Epoch 49 , Train Loss 422.7832336425781 , Test Loss 421.783203125\n",
      "Epoch 50 , Train Loss 421.783203125 , Test Loss 420.7831726074219\n",
      "Epoch 51 , Train Loss 420.7831726074219 , Test Loss 419.7832336425781\n",
      "Epoch 52 , Train Loss 419.7832336425781 , Test Loss 418.7832336425781\n",
      "Epoch 53 , Train Loss 418.7832336425781 , Test Loss 417.783203125\n",
      "Epoch 54 , Train Loss 417.783203125 , Test Loss 416.7831726074219\n",
      "Epoch 55 , Train Loss 416.7831726074219 , Test Loss 415.7832336425781\n",
      "Epoch 56 , Train Loss 415.7832336425781 , Test Loss 414.7831726074219\n",
      "Epoch 57 , Train Loss 414.7831726074219 , Test Loss 413.7831726074219\n",
      "Epoch 58 , Train Loss 413.7831726074219 , Test Loss 412.7831726074219\n",
      "Epoch 59 , Train Loss 412.7831726074219 , Test Loss 411.7832336425781\n",
      "Epoch 60 , Train Loss 411.7832336425781 , Test Loss 410.7832336425781\n",
      "Epoch 61 , Train Loss 410.7832336425781 , Test Loss 409.7832336425781\n",
      "Epoch 62 , Train Loss 409.7832336425781 , Test Loss 408.7831726074219\n",
      "Epoch 63 , Train Loss 408.7831726074219 , Test Loss 407.7832336425781\n",
      "Epoch 64 , Train Loss 407.7832336425781 , Test Loss 406.7831726074219\n",
      "Epoch 65 , Train Loss 406.7831726074219 , Test Loss 405.783203125\n",
      "Epoch 66 , Train Loss 405.783203125 , Test Loss 404.783203125\n",
      "Epoch 67 , Train Loss 404.783203125 , Test Loss 403.7832336425781\n",
      "Epoch 68 , Train Loss 403.7832336425781 , Test Loss 402.7832336425781\n",
      "Epoch 69 , Train Loss 402.7832336425781 , Test Loss 401.783203125\n",
      "Epoch 70 , Train Loss 401.783203125 , Test Loss 400.7831726074219\n",
      "Epoch 71 , Train Loss 400.7831726074219 , Test Loss 399.7832336425781\n",
      "Epoch 72 , Train Loss 399.7832336425781 , Test Loss 398.7832336425781\n",
      "Epoch 73 , Train Loss 398.7832336425781 , Test Loss 397.7831726074219\n",
      "Epoch 74 , Train Loss 397.7831726074219 , Test Loss 396.7831726074219\n",
      "Epoch 75 , Train Loss 396.7831726074219 , Test Loss 395.7832336425781\n",
      "Epoch 76 , Train Loss 395.7832336425781 , Test Loss 394.7831726074219\n",
      "Epoch 77 , Train Loss 394.7831726074219 , Test Loss 393.7832336425781\n",
      "Epoch 78 , Train Loss 393.7832336425781 , Test Loss 392.783203125\n",
      "Epoch 79 , Train Loss 392.783203125 , Test Loss 391.7832336425781\n",
      "Epoch 80 , Train Loss 391.7832336425781 , Test Loss 390.7831726074219\n",
      "Epoch 81 , Train Loss 390.7831726074219 , Test Loss 389.783203125\n",
      "Epoch 82 , Train Loss 389.783203125 , Test Loss 388.7831726074219\n",
      "Epoch 83 , Train Loss 388.7831726074219 , Test Loss 387.7831726074219\n",
      "Epoch 84 , Train Loss 387.7831726074219 , Test Loss 386.7831726074219\n",
      "Epoch 85 , Train Loss 386.7831726074219 , Test Loss 385.783203125\n",
      "Epoch 86 , Train Loss 385.783203125 , Test Loss 384.7831726074219\n",
      "Epoch 87 , Train Loss 384.7831726074219 , Test Loss 383.783203125\n",
      "Epoch 88 , Train Loss 383.783203125 , Test Loss 382.7831726074219\n",
      "Epoch 89 , Train Loss 382.7831726074219 , Test Loss 381.783203125\n",
      "Epoch 90 , Train Loss 381.783203125 , Test Loss 380.7831726074219\n",
      "Epoch 91 , Train Loss 380.7831726074219 , Test Loss 379.783203125\n",
      "Epoch 92 , Train Loss 379.783203125 , Test Loss 378.7831726074219\n",
      "Epoch 93 , Train Loss 378.7831726074219 , Test Loss 377.78314208984375\n",
      "Epoch 94 , Train Loss 377.78314208984375 , Test Loss 376.7831726074219\n",
      "Epoch 95 , Train Loss 376.7831726074219 , Test Loss 375.7832336425781\n",
      "Epoch 96 , Train Loss 375.7832336425781 , Test Loss 374.783203125\n",
      "Epoch 97 , Train Loss 374.783203125 , Test Loss 373.78314208984375\n",
      "Epoch 98 , Train Loss 373.78314208984375 , Test Loss 372.7831726074219\n",
      "Epoch 99 , Train Loss 372.7831726074219 , Test Loss 371.783203125\n",
      "Epoch 100 , Train Loss 371.783203125 , Test Loss 370.7831726074219\n",
      "Epoch 101 , Train Loss 370.7831726074219 , Test Loss 369.783203125\n",
      "Epoch 102 , Train Loss 369.783203125 , Test Loss 368.7831726074219\n",
      "Epoch 103 , Train Loss 368.7831726074219 , Test Loss 367.783203125\n",
      "Epoch 104 , Train Loss 367.783203125 , Test Loss 366.7832336425781\n",
      "Epoch 105 , Train Loss 366.7832336425781 , Test Loss 365.78314208984375\n",
      "Epoch 106 , Train Loss 365.78314208984375 , Test Loss 364.7831726074219\n",
      "Epoch 107 , Train Loss 364.7831726074219 , Test Loss 363.7831726074219\n",
      "Epoch 108 , Train Loss 363.7831726074219 , Test Loss 362.783203125\n",
      "Epoch 109 , Train Loss 362.783203125 , Test Loss 361.78314208984375\n",
      "Epoch 110 , Train Loss 361.78314208984375 , Test Loss 360.7831726074219\n",
      "Epoch 111 , Train Loss 360.7831726074219 , Test Loss 359.783203125\n",
      "Epoch 112 , Train Loss 359.783203125 , Test Loss 358.783203125\n",
      "Epoch 113 , Train Loss 358.783203125 , Test Loss 357.783203125\n",
      "Epoch 114 , Train Loss 357.783203125 , Test Loss 356.7831726074219\n",
      "Epoch 115 , Train Loss 356.7831726074219 , Test Loss 355.7832336425781\n",
      "Epoch 116 , Train Loss 355.7832336425781 , Test Loss 354.7831726074219\n",
      "Epoch 117 , Train Loss 354.7831726074219 , Test Loss 353.783203125\n",
      "Epoch 118 , Train Loss 353.783203125 , Test Loss 352.7831726074219\n",
      "Epoch 119 , Train Loss 352.7831726074219 , Test Loss 351.7832336425781\n",
      "Epoch 120 , Train Loss 351.7832336425781 , Test Loss 350.7831726074219\n",
      "Epoch 121 , Train Loss 350.7831726074219 , Test Loss 349.783203125\n",
      "Epoch 122 , Train Loss 349.783203125 , Test Loss 348.7831726074219\n",
      "Epoch 123 , Train Loss 348.7831726074219 , Test Loss 347.7832336425781\n",
      "Epoch 124 , Train Loss 347.7832336425781 , Test Loss 346.7831726074219\n",
      "Epoch 125 , Train Loss 346.7831726074219 , Test Loss 345.783203125\n",
      "Epoch 126 , Train Loss 345.783203125 , Test Loss 344.7831726074219\n",
      "Epoch 127 , Train Loss 344.7831726074219 , Test Loss 343.7832336425781\n",
      "Epoch 128 , Train Loss 343.7832336425781 , Test Loss 342.7831726074219\n",
      "Epoch 129 , Train Loss 342.7831726074219 , Test Loss 341.783203125\n",
      "Epoch 130 , Train Loss 341.783203125 , Test Loss 340.7831726074219\n",
      "Epoch 131 , Train Loss 340.7831726074219 , Test Loss 339.7832336425781\n",
      "Epoch 132 , Train Loss 339.7832336425781 , Test Loss 338.7831726074219\n",
      "Epoch 133 , Train Loss 338.7831726074219 , Test Loss 337.783203125\n",
      "Epoch 134 , Train Loss 337.783203125 , Test Loss 336.7831726074219\n",
      "Epoch 135 , Train Loss 336.7831726074219 , Test Loss 335.7832336425781\n",
      "Epoch 136 , Train Loss 335.7832336425781 , Test Loss 334.7831726074219\n",
      "Epoch 137 , Train Loss 334.7831726074219 , Test Loss 333.783203125\n",
      "Epoch 138 , Train Loss 333.783203125 , Test Loss 332.7831726074219\n",
      "Epoch 139 , Train Loss 332.7831726074219 , Test Loss 331.7832336425781\n",
      "Epoch 140 , Train Loss 331.7832336425781 , Test Loss 330.7831726074219\n",
      "Epoch 141 , Train Loss 330.7831726074219 , Test Loss 329.783203125\n",
      "Epoch 142 , Train Loss 329.783203125 , Test Loss 328.7831726074219\n",
      "Epoch 143 , Train Loss 328.7831726074219 , Test Loss 327.7832336425781\n",
      "Epoch 144 , Train Loss 327.7832336425781 , Test Loss 326.7831726074219\n",
      "Epoch 145 , Train Loss 326.7831726074219 , Test Loss 325.783203125\n",
      "Epoch 146 , Train Loss 325.783203125 , Test Loss 324.7831726074219\n",
      "Epoch 147 , Train Loss 324.7831726074219 , Test Loss 323.7832336425781\n",
      "Epoch 148 , Train Loss 323.7832336425781 , Test Loss 322.7831726074219\n",
      "Epoch 149 , Train Loss 322.7831726074219 , Test Loss 321.783203125\n",
      "Epoch 150 , Train Loss 321.783203125 , Test Loss 320.7831726074219\n",
      "Epoch 151 , Train Loss 320.7831726074219 , Test Loss 319.783203125\n",
      "Epoch 152 , Train Loss 319.783203125 , Test Loss 318.7831726074219\n",
      "Epoch 153 , Train Loss 318.7831726074219 , Test Loss 317.783203125\n",
      "Epoch 154 , Train Loss 317.783203125 , Test Loss 316.7831726074219\n",
      "Epoch 155 , Train Loss 316.7831726074219 , Test Loss 315.7832336425781\n",
      "Epoch 156 , Train Loss 315.7832336425781 , Test Loss 314.7831726074219\n",
      "Epoch 157 , Train Loss 314.7831726074219 , Test Loss 313.7831726074219\n",
      "Epoch 158 , Train Loss 313.7831726074219 , Test Loss 312.7831726074219\n",
      "Epoch 159 , Train Loss 312.7831726074219 , Test Loss 311.783203125\n",
      "Epoch 160 , Train Loss 311.783203125 , Test Loss 310.7831726074219\n",
      "Epoch 161 , Train Loss 310.7831726074219 , Test Loss 309.783203125\n",
      "Epoch 162 , Train Loss 309.783203125 , Test Loss 308.7831726074219\n",
      "Epoch 163 , Train Loss 308.7831726074219 , Test Loss 307.7832336425781\n",
      "Epoch 164 , Train Loss 307.7832336425781 , Test Loss 306.7831726074219\n",
      "Epoch 165 , Train Loss 306.7831726074219 , Test Loss 305.783203125\n",
      "Epoch 166 , Train Loss 305.783203125 , Test Loss 304.7831726074219\n",
      "Epoch 167 , Train Loss 304.7831726074219 , Test Loss 303.783203125\n",
      "Epoch 168 , Train Loss 303.783203125 , Test Loss 302.7831726074219\n",
      "Epoch 169 , Train Loss 302.7831726074219 , Test Loss 301.783203125\n",
      "Epoch 170 , Train Loss 301.783203125 , Test Loss 300.7831726074219\n",
      "Epoch 171 , Train Loss 300.7831726074219 , Test Loss 299.7832336425781\n",
      "Epoch 172 , Train Loss 299.7832336425781 , Test Loss 298.7831726074219\n",
      "Epoch 173 , Train Loss 298.7831726074219 , Test Loss 297.783203125\n",
      "Epoch 174 , Train Loss 297.783203125 , Test Loss 296.7831726074219\n",
      "Epoch 175 , Train Loss 296.7831726074219 , Test Loss 295.783203125\n",
      "Epoch 176 , Train Loss 295.783203125 , Test Loss 294.7831726074219\n",
      "Epoch 177 , Train Loss 294.7831726074219 , Test Loss 293.783203125\n",
      "Epoch 178 , Train Loss 293.783203125 , Test Loss 292.7831726074219\n",
      "Epoch 179 , Train Loss 292.7831726074219 , Test Loss 291.7832336425781\n",
      "Epoch 180 , Train Loss 291.7832336425781 , Test Loss 290.7831726074219\n",
      "Epoch 181 , Train Loss 290.7831726074219 , Test Loss 289.783203125\n",
      "Epoch 182 , Train Loss 289.783203125 , Test Loss 288.7831726074219\n",
      "Epoch 183 , Train Loss 288.7831726074219 , Test Loss 287.7832336425781\n",
      "Epoch 184 , Train Loss 287.7832336425781 , Test Loss 286.7831726074219\n",
      "Epoch 185 , Train Loss 286.7831726074219 , Test Loss 285.783203125\n",
      "Epoch 186 , Train Loss 285.783203125 , Test Loss 284.7831726074219\n",
      "Epoch 187 , Train Loss 284.7831726074219 , Test Loss 283.7832336425781\n",
      "Epoch 188 , Train Loss 283.7832336425781 , Test Loss 282.7831726074219\n",
      "Epoch 189 , Train Loss 282.7831726074219 , Test Loss 281.783203125\n",
      "Epoch 190 , Train Loss 281.783203125 , Test Loss 280.7831726074219\n",
      "Epoch 191 , Train Loss 280.7831726074219 , Test Loss 279.7832336425781\n",
      "Epoch 192 , Train Loss 279.7832336425781 , Test Loss 278.7831726074219\n",
      "Epoch 193 , Train Loss 278.7831726074219 , Test Loss 277.783203125\n",
      "Epoch 194 , Train Loss 277.783203125 , Test Loss 276.7831726074219\n",
      "Epoch 195 , Train Loss 276.7831726074219 , Test Loss 275.7832336425781\n",
      "Epoch 196 , Train Loss 275.7832336425781 , Test Loss 274.7831726074219\n",
      "Epoch 197 , Train Loss 274.7831726074219 , Test Loss 273.783203125\n",
      "Epoch 198 , Train Loss 273.783203125 , Test Loss 272.7831726074219\n",
      "Epoch 199 , Train Loss 272.7831726074219 , Test Loss 271.7832336425781\n",
      "Epoch 200 , Train Loss 271.7832336425781 , Test Loss 270.7831726074219\n",
      "Epoch 201 , Train Loss 270.7831726074219 , Test Loss 269.783203125\n",
      "Epoch 202 , Train Loss 269.783203125 , Test Loss 268.7831726074219\n",
      "Epoch 203 , Train Loss 268.7831726074219 , Test Loss 267.7832336425781\n",
      "Epoch 204 , Train Loss 267.7832336425781 , Test Loss 266.7831726074219\n",
      "Epoch 205 , Train Loss 266.7831726074219 , Test Loss 265.783203125\n",
      "Epoch 206 , Train Loss 265.783203125 , Test Loss 264.7831726074219\n",
      "Epoch 207 , Train Loss 264.7831726074219 , Test Loss 263.7832336425781\n",
      "Epoch 208 , Train Loss 263.7832336425781 , Test Loss 262.7831726074219\n",
      "Epoch 209 , Train Loss 262.7831726074219 , Test Loss 261.783203125\n",
      "Epoch 210 , Train Loss 261.783203125 , Test Loss 260.7831726074219\n",
      "Epoch 211 , Train Loss 260.7831726074219 , Test Loss 259.7832336425781\n",
      "Epoch 212 , Train Loss 259.7832336425781 , Test Loss 258.7831726074219\n",
      "Epoch 213 , Train Loss 258.7831726074219 , Test Loss 257.783203125\n",
      "Epoch 214 , Train Loss 257.783203125 , Test Loss 256.7831726074219\n",
      "Epoch 215 , Train Loss 256.7831726074219 , Test Loss 255.78321838378906\n",
      "Epoch 216 , Train Loss 255.78321838378906 , Test Loss 254.78318786621094\n",
      "Epoch 217 , Train Loss 254.78318786621094 , Test Loss 253.78318786621094\n",
      "Epoch 218 , Train Loss 253.78318786621094 , Test Loss 252.78318786621094\n",
      "Epoch 219 , Train Loss 252.78318786621094 , Test Loss 251.78321838378906\n",
      "Epoch 220 , Train Loss 251.78321838378906 , Test Loss 250.78318786621094\n",
      "Epoch 221 , Train Loss 250.78318786621094 , Test Loss 249.78318786621094\n",
      "Epoch 222 , Train Loss 249.78318786621094 , Test Loss 248.78318786621094\n",
      "Epoch 223 , Train Loss 248.78318786621094 , Test Loss 247.78321838378906\n",
      "Epoch 224 , Train Loss 247.78321838378906 , Test Loss 246.78318786621094\n",
      "Epoch 225 , Train Loss 246.78318786621094 , Test Loss 245.78318786621094\n",
      "Epoch 226 , Train Loss 245.78318786621094 , Test Loss 244.78318786621094\n",
      "Epoch 227 , Train Loss 244.78318786621094 , Test Loss 243.78321838378906\n",
      "Epoch 228 , Train Loss 243.78321838378906 , Test Loss 242.78318786621094\n",
      "Epoch 229 , Train Loss 242.78318786621094 , Test Loss 241.78318786621094\n",
      "Epoch 230 , Train Loss 241.78318786621094 , Test Loss 240.78318786621094\n",
      "Epoch 231 , Train Loss 240.78318786621094 , Test Loss 239.78321838378906\n",
      "Epoch 232 , Train Loss 239.78321838378906 , Test Loss 238.78318786621094\n",
      "Epoch 233 , Train Loss 238.78318786621094 , Test Loss 237.78318786621094\n",
      "Epoch 234 , Train Loss 237.78318786621094 , Test Loss 236.78318786621094\n",
      "Epoch 235 , Train Loss 236.78318786621094 , Test Loss 235.78318786621094\n",
      "Epoch 236 , Train Loss 235.78318786621094 , Test Loss 234.78318786621094\n",
      "Epoch 237 , Train Loss 234.78318786621094 , Test Loss 233.78318786621094\n",
      "Epoch 238 , Train Loss 233.78318786621094 , Test Loss 232.78318786621094\n",
      "Epoch 239 , Train Loss 232.78318786621094 , Test Loss 231.78318786621094\n",
      "Epoch 240 , Train Loss 231.78318786621094 , Test Loss 230.78318786621094\n",
      "Epoch 241 , Train Loss 230.78318786621094 , Test Loss 229.78318786621094\n",
      "Epoch 242 , Train Loss 229.78318786621094 , Test Loss 228.78318786621094\n",
      "Epoch 243 , Train Loss 228.78318786621094 , Test Loss 227.78318786621094\n",
      "Epoch 244 , Train Loss 227.78318786621094 , Test Loss 226.78318786621094\n",
      "Epoch 245 , Train Loss 226.78318786621094 , Test Loss 225.78318786621094\n",
      "Epoch 246 , Train Loss 225.78318786621094 , Test Loss 224.78318786621094\n",
      "Epoch 247 , Train Loss 224.78318786621094 , Test Loss 223.78318786621094\n",
      "Epoch 248 , Train Loss 223.78318786621094 , Test Loss 222.78318786621094\n",
      "Epoch 249 , Train Loss 222.78318786621094 , Test Loss 221.78318786621094\n",
      "Epoch 250 , Train Loss 221.78318786621094 , Test Loss 220.78318786621094\n",
      "Epoch 251 , Train Loss 220.78318786621094 , Test Loss 219.78317260742188\n",
      "Epoch 252 , Train Loss 219.78317260742188 , Test Loss 218.78318786621094\n",
      "Epoch 253 , Train Loss 218.78318786621094 , Test Loss 217.78318786621094\n",
      "Epoch 254 , Train Loss 217.78318786621094 , Test Loss 216.78318786621094\n",
      "Epoch 255 , Train Loss 216.78318786621094 , Test Loss 215.78317260742188\n",
      "Epoch 256 , Train Loss 215.78317260742188 , Test Loss 214.78318786621094\n",
      "Epoch 257 , Train Loss 214.78318786621094 , Test Loss 213.78318786621094\n",
      "Epoch 258 , Train Loss 213.78318786621094 , Test Loss 212.78318786621094\n",
      "Epoch 259 , Train Loss 212.78318786621094 , Test Loss 211.78318786621094\n",
      "Epoch 260 , Train Loss 211.78318786621094 , Test Loss 210.78318786621094\n",
      "Epoch 261 , Train Loss 210.78318786621094 , Test Loss 209.78318786621094\n",
      "Epoch 262 , Train Loss 209.78318786621094 , Test Loss 208.78318786621094\n",
      "Epoch 263 , Train Loss 208.78318786621094 , Test Loss 207.78318786621094\n",
      "Epoch 264 , Train Loss 207.78318786621094 , Test Loss 206.78318786621094\n",
      "Epoch 265 , Train Loss 206.78318786621094 , Test Loss 205.78318786621094\n",
      "Epoch 266 , Train Loss 205.78318786621094 , Test Loss 204.78318786621094\n",
      "Epoch 267 , Train Loss 204.78318786621094 , Test Loss 203.78318786621094\n",
      "Epoch 268 , Train Loss 203.78318786621094 , Test Loss 202.78318786621094\n",
      "Epoch 269 , Train Loss 202.78318786621094 , Test Loss 201.78318786621094\n",
      "Epoch 270 , Train Loss 201.78318786621094 , Test Loss 200.78318786621094\n",
      "Epoch 271 , Train Loss 200.78318786621094 , Test Loss 199.783203125\n",
      "Epoch 272 , Train Loss 199.783203125 , Test Loss 198.78318786621094\n",
      "Epoch 273 , Train Loss 198.78318786621094 , Test Loss 197.78318786621094\n",
      "Epoch 274 , Train Loss 197.78318786621094 , Test Loss 196.78318786621094\n",
      "Epoch 275 , Train Loss 196.78318786621094 , Test Loss 195.78318786621094\n",
      "Epoch 276 , Train Loss 195.78318786621094 , Test Loss 194.78318786621094\n",
      "Epoch 277 , Train Loss 194.78318786621094 , Test Loss 193.78317260742188\n",
      "Epoch 278 , Train Loss 193.78317260742188 , Test Loss 192.78318786621094\n",
      "Epoch 279 , Train Loss 192.78318786621094 , Test Loss 191.78318786621094\n",
      "Epoch 280 , Train Loss 191.78318786621094 , Test Loss 190.78318786621094\n",
      "Epoch 281 , Train Loss 190.78318786621094 , Test Loss 189.78318786621094\n",
      "Epoch 282 , Train Loss 189.78318786621094 , Test Loss 188.78318786621094\n",
      "Epoch 283 , Train Loss 188.78318786621094 , Test Loss 187.78318786621094\n",
      "Epoch 284 , Train Loss 187.78318786621094 , Test Loss 186.78318786621094\n",
      "Epoch 285 , Train Loss 186.78318786621094 , Test Loss 185.78318786621094\n",
      "Epoch 286 , Train Loss 185.78318786621094 , Test Loss 184.78318786621094\n",
      "Epoch 287 , Train Loss 184.78318786621094 , Test Loss 183.78318786621094\n",
      "Epoch 288 , Train Loss 183.78318786621094 , Test Loss 182.78318786621094\n",
      "Epoch 289 , Train Loss 182.78318786621094 , Test Loss 181.78318786621094\n",
      "Epoch 290 , Train Loss 181.78318786621094 , Test Loss 180.78318786621094\n",
      "Epoch 291 , Train Loss 180.78318786621094 , Test Loss 179.78318786621094\n",
      "Epoch 292 , Train Loss 179.78318786621094 , Test Loss 178.78318786621094\n",
      "Epoch 293 , Train Loss 178.78318786621094 , Test Loss 177.78318786621094\n",
      "Epoch 294 , Train Loss 177.78318786621094 , Test Loss 176.78318786621094\n",
      "Epoch 295 , Train Loss 176.78318786621094 , Test Loss 175.78318786621094\n",
      "Epoch 296 , Train Loss 175.78318786621094 , Test Loss 174.78318786621094\n",
      "Epoch 297 , Train Loss 174.78318786621094 , Test Loss 173.78318786621094\n",
      "Epoch 298 , Train Loss 173.78318786621094 , Test Loss 172.78318786621094\n",
      "Epoch 299 , Train Loss 172.78318786621094 , Test Loss 171.78318786621094\n",
      "Epoch 300 , Train Loss 171.78318786621094 , Test Loss 170.78318786621094\n",
      "Epoch 301 , Train Loss 170.78318786621094 , Test Loss 169.78318786621094\n",
      "Epoch 302 , Train Loss 169.78318786621094 , Test Loss 168.78318786621094\n",
      "Epoch 303 , Train Loss 168.78318786621094 , Test Loss 167.78318786621094\n",
      "Epoch 304 , Train Loss 167.78318786621094 , Test Loss 166.78318786621094\n",
      "Epoch 305 , Train Loss 166.78318786621094 , Test Loss 165.78318786621094\n",
      "Epoch 306 , Train Loss 165.78318786621094 , Test Loss 164.78318786621094\n",
      "Epoch 307 , Train Loss 164.78318786621094 , Test Loss 163.78318786621094\n",
      "Epoch 308 , Train Loss 163.78318786621094 , Test Loss 162.78318786621094\n",
      "Epoch 309 , Train Loss 162.78318786621094 , Test Loss 161.78318786621094\n",
      "Epoch 310 , Train Loss 161.78318786621094 , Test Loss 160.78318786621094\n",
      "Epoch 311 , Train Loss 160.78318786621094 , Test Loss 159.78318786621094\n",
      "Epoch 312 , Train Loss 159.78318786621094 , Test Loss 158.78318786621094\n",
      "Epoch 313 , Train Loss 158.78318786621094 , Test Loss 157.78318786621094\n",
      "Epoch 314 , Train Loss 157.78318786621094 , Test Loss 156.78317260742188\n",
      "Epoch 315 , Train Loss 156.78317260742188 , Test Loss 155.78318786621094\n",
      "Epoch 316 , Train Loss 155.78318786621094 , Test Loss 154.78318786621094\n",
      "Epoch 317 , Train Loss 154.78318786621094 , Test Loss 153.78318786621094\n",
      "Epoch 318 , Train Loss 153.78318786621094 , Test Loss 152.78318786621094\n",
      "Epoch 319 , Train Loss 152.78318786621094 , Test Loss 151.78318786621094\n",
      "Epoch 320 , Train Loss 151.78318786621094 , Test Loss 150.78318786621094\n",
      "Epoch 321 , Train Loss 150.78318786621094 , Test Loss 149.78318786621094\n",
      "Epoch 322 , Train Loss 149.78318786621094 , Test Loss 148.78318786621094\n",
      "Epoch 323 , Train Loss 148.78318786621094 , Test Loss 147.78318786621094\n",
      "Epoch 324 , Train Loss 147.78318786621094 , Test Loss 146.78318786621094\n",
      "Epoch 325 , Train Loss 146.78318786621094 , Test Loss 145.78318786621094\n",
      "Epoch 326 , Train Loss 145.78318786621094 , Test Loss 144.78318786621094\n",
      "Epoch 327 , Train Loss 144.78318786621094 , Test Loss 143.78318786621094\n",
      "Epoch 328 , Train Loss 143.78318786621094 , Test Loss 142.78318786621094\n",
      "Epoch 329 , Train Loss 142.78318786621094 , Test Loss 141.78318786621094\n",
      "Epoch 330 , Train Loss 141.78318786621094 , Test Loss 140.78318786621094\n",
      "Epoch 331 , Train Loss 140.78318786621094 , Test Loss 139.78318786621094\n",
      "Epoch 332 , Train Loss 139.78318786621094 , Test Loss 138.78318786621094\n",
      "Epoch 333 , Train Loss 138.78318786621094 , Test Loss 137.78318786621094\n",
      "Epoch 334 , Train Loss 137.78318786621094 , Test Loss 136.78318786621094\n",
      "Epoch 335 , Train Loss 136.78318786621094 , Test Loss 135.78318786621094\n",
      "Epoch 336 , Train Loss 135.78318786621094 , Test Loss 134.78318786621094\n",
      "Epoch 337 , Train Loss 134.78318786621094 , Test Loss 133.78318786621094\n",
      "Epoch 338 , Train Loss 133.78318786621094 , Test Loss 132.78318786621094\n",
      "Epoch 339 , Train Loss 132.78318786621094 , Test Loss 131.78318786621094\n",
      "Epoch 340 , Train Loss 131.78318786621094 , Test Loss 130.78318786621094\n",
      "Epoch 341 , Train Loss 130.78318786621094 , Test Loss 129.78318786621094\n",
      "Epoch 342 , Train Loss 129.78318786621094 , Test Loss 128.78318786621094\n",
      "Epoch 343 , Train Loss 128.78318786621094 , Test Loss 127.7831802368164\n",
      "Epoch 344 , Train Loss 127.7831802368164 , Test Loss 126.78316497802734\n",
      "Epoch 345 , Train Loss 126.78316497802734 , Test Loss 125.7831802368164\n",
      "Epoch 346 , Train Loss 125.7831802368164 , Test Loss 124.7831802368164\n",
      "Epoch 347 , Train Loss 124.7831802368164 , Test Loss 123.7831802368164\n",
      "Epoch 348 , Train Loss 123.7831802368164 , Test Loss 122.78316497802734\n",
      "Epoch 349 , Train Loss 122.78316497802734 , Test Loss 121.7831802368164\n",
      "Epoch 350 , Train Loss 121.7831802368164 , Test Loss 120.78316497802734\n",
      "Epoch 351 , Train Loss 120.78316497802734 , Test Loss 119.7831802368164\n",
      "Epoch 352 , Train Loss 119.7831802368164 , Test Loss 118.78316497802734\n",
      "Epoch 353 , Train Loss 118.78316497802734 , Test Loss 117.7831802368164\n",
      "Epoch 354 , Train Loss 117.7831802368164 , Test Loss 116.78316497802734\n",
      "Epoch 355 , Train Loss 116.78316497802734 , Test Loss 115.7831802368164\n",
      "Epoch 356 , Train Loss 115.7831802368164 , Test Loss 114.78316497802734\n",
      "Epoch 357 , Train Loss 114.78316497802734 , Test Loss 113.7831802368164\n",
      "Epoch 358 , Train Loss 113.7831802368164 , Test Loss 112.7831802368164\n",
      "Epoch 359 , Train Loss 112.7831802368164 , Test Loss 111.7831802368164\n",
      "Epoch 360 , Train Loss 111.7831802368164 , Test Loss 110.7831802368164\n",
      "Epoch 361 , Train Loss 110.7831802368164 , Test Loss 109.7831802368164\n",
      "Epoch 362 , Train Loss 109.7831802368164 , Test Loss 108.7831802368164\n",
      "Epoch 363 , Train Loss 108.7831802368164 , Test Loss 107.7831802368164\n",
      "Epoch 364 , Train Loss 107.7831802368164 , Test Loss 106.78317260742188\n",
      "Epoch 365 , Train Loss 106.78317260742188 , Test Loss 105.7831802368164\n",
      "Epoch 366 , Train Loss 105.7831802368164 , Test Loss 104.78317260742188\n",
      "Epoch 367 , Train Loss 104.78317260742188 , Test Loss 103.7831802368164\n",
      "Epoch 368 , Train Loss 103.7831802368164 , Test Loss 102.78316497802734\n",
      "Epoch 369 , Train Loss 102.78316497802734 , Test Loss 101.7831802368164\n",
      "Epoch 370 , Train Loss 101.7831802368164 , Test Loss 100.78316497802734\n",
      "Epoch 371 , Train Loss 100.78316497802734 , Test Loss 99.7831802368164\n",
      "Epoch 372 , Train Loss 99.7831802368164 , Test Loss 98.78316497802734\n",
      "Epoch 373 , Train Loss 98.78316497802734 , Test Loss 97.7831802368164\n",
      "Epoch 374 , Train Loss 97.7831802368164 , Test Loss 96.78316497802734\n",
      "Epoch 375 , Train Loss 96.78316497802734 , Test Loss 95.7831802368164\n",
      "Epoch 376 , Train Loss 95.7831802368164 , Test Loss 94.78316497802734\n",
      "Epoch 377 , Train Loss 94.78316497802734 , Test Loss 93.7831802368164\n",
      "Epoch 378 , Train Loss 93.7831802368164 , Test Loss 92.78316497802734\n",
      "Epoch 379 , Train Loss 92.78316497802734 , Test Loss 91.7831802368164\n",
      "Epoch 380 , Train Loss 91.7831802368164 , Test Loss 90.78316497802734\n",
      "Epoch 381 , Train Loss 90.78316497802734 , Test Loss 89.78316497802734\n",
      "Epoch 382 , Train Loss 89.78316497802734 , Test Loss 88.78316497802734\n",
      "Epoch 383 , Train Loss 88.78316497802734 , Test Loss 87.78317260742188\n",
      "Epoch 384 , Train Loss 87.78317260742188 , Test Loss 86.78316497802734\n",
      "Epoch 385 , Train Loss 86.78316497802734 , Test Loss 85.78317260742188\n",
      "Epoch 386 , Train Loss 85.78317260742188 , Test Loss 84.78316497802734\n",
      "Epoch 387 , Train Loss 84.78316497802734 , Test Loss 83.78317260742188\n",
      "Epoch 388 , Train Loss 83.78317260742188 , Test Loss 82.78316497802734\n",
      "Epoch 389 , Train Loss 82.78316497802734 , Test Loss 81.78317260742188\n",
      "Epoch 390 , Train Loss 81.78317260742188 , Test Loss 80.78316497802734\n",
      "Epoch 391 , Train Loss 80.78316497802734 , Test Loss 79.78317260742188\n",
      "Epoch 392 , Train Loss 79.78317260742188 , Test Loss 78.78316497802734\n",
      "Epoch 393 , Train Loss 78.78316497802734 , Test Loss 77.78316497802734\n",
      "Epoch 394 , Train Loss 77.78316497802734 , Test Loss 76.78316497802734\n",
      "Epoch 395 , Train Loss 76.78316497802734 , Test Loss 75.78317260742188\n",
      "Epoch 396 , Train Loss 75.78317260742188 , Test Loss 74.78317260742188\n",
      "Epoch 397 , Train Loss 74.78317260742188 , Test Loss 73.78317260742188\n",
      "Epoch 398 , Train Loss 73.78317260742188 , Test Loss 72.78317260742188\n",
      "Epoch 399 , Train Loss 72.78317260742188 , Test Loss 71.78317260742188\n",
      "Epoch 400 , Train Loss 71.78317260742188 , Test Loss 70.78317260742188\n",
      "Epoch 401 , Train Loss 70.78317260742188 , Test Loss 69.78317260742188\n",
      "Epoch 402 , Train Loss 69.78317260742188 , Test Loss 68.78317260742188\n",
      "Epoch 403 , Train Loss 68.78317260742188 , Test Loss 67.78317260742188\n",
      "Epoch 404 , Train Loss 67.78317260742188 , Test Loss 66.78317260742188\n",
      "Epoch 405 , Train Loss 66.78317260742188 , Test Loss 65.7831802368164\n",
      "Epoch 406 , Train Loss 65.7831802368164 , Test Loss 64.78317260742188\n",
      "Epoch 407 , Train Loss 64.78317260742188 , Test Loss 63.783172607421875\n",
      "Epoch 408 , Train Loss 63.783172607421875 , Test Loss 62.783172607421875\n",
      "Epoch 409 , Train Loss 62.783172607421875 , Test Loss 61.783172607421875\n",
      "Epoch 410 , Train Loss 61.783172607421875 , Test Loss 60.783172607421875\n",
      "Epoch 411 , Train Loss 60.783172607421875 , Test Loss 59.783172607421875\n",
      "Epoch 412 , Train Loss 59.783172607421875 , Test Loss 58.783172607421875\n",
      "Epoch 413 , Train Loss 58.783172607421875 , Test Loss 57.783172607421875\n",
      "Epoch 414 , Train Loss 57.783172607421875 , Test Loss 56.783172607421875\n",
      "Epoch 415 , Train Loss 56.783172607421875 , Test Loss 55.783172607421875\n",
      "Epoch 416 , Train Loss 55.783172607421875 , Test Loss 54.78316879272461\n",
      "Epoch 417 , Train Loss 54.78316879272461 , Test Loss 53.783172607421875\n",
      "Epoch 418 , Train Loss 53.783172607421875 , Test Loss 52.783172607421875\n",
      "Epoch 419 , Train Loss 52.783172607421875 , Test Loss 51.783180236816406\n",
      "Epoch 420 , Train Loss 51.783180236816406 , Test Loss 50.783180236816406\n",
      "Epoch 421 , Train Loss 50.783180236816406 , Test Loss 49.783180236816406\n",
      "Epoch 422 , Train Loss 49.783180236816406 , Test Loss 48.783180236816406\n",
      "Epoch 423 , Train Loss 48.783180236816406 , Test Loss 47.78317642211914\n",
      "Epoch 424 , Train Loss 47.78317642211914 , Test Loss 46.78317642211914\n",
      "Epoch 425 , Train Loss 46.78317642211914 , Test Loss 45.78317642211914\n",
      "Epoch 426 , Train Loss 45.78317642211914 , Test Loss 44.78317642211914\n",
      "Epoch 427 , Train Loss 44.78317642211914 , Test Loss 43.783180236816406\n",
      "Epoch 428 , Train Loss 43.783180236816406 , Test Loss 42.783180236816406\n",
      "Epoch 429 , Train Loss 42.783180236816406 , Test Loss 41.78318786621094\n",
      "Epoch 430 , Train Loss 41.78318786621094 , Test Loss 40.783199310302734\n",
      "Epoch 431 , Train Loss 40.783199310302734 , Test Loss 39.7832145690918\n",
      "Epoch 432 , Train Loss 39.7832145690918 , Test Loss 38.78327941894531\n",
      "Epoch 433 , Train Loss 38.78327941894531 , Test Loss 37.78335189819336\n",
      "Epoch 434 , Train Loss 37.78335189819336 , Test Loss 36.78348922729492\n",
      "Epoch 435 , Train Loss 36.78348922729492 , Test Loss 35.783687591552734\n",
      "Epoch 436 , Train Loss 35.783687591552734 , Test Loss 34.78396987915039\n",
      "Epoch 437 , Train Loss 34.78396987915039 , Test Loss 33.78439712524414\n",
      "Epoch 438 , Train Loss 33.78439712524414 , Test Loss 32.78508377075195\n",
      "Epoch 439 , Train Loss 32.78508377075195 , Test Loss 31.78611183166504\n",
      "Epoch 440 , Train Loss 31.78611183166504 , Test Loss 30.787660598754883\n",
      "Epoch 441 , Train Loss 30.787660598754883 , Test Loss 29.789913177490234\n",
      "Epoch 442 , Train Loss 29.789913177490234 , Test Loss 28.79315185546875\n",
      "Epoch 443 , Train Loss 28.79315185546875 , Test Loss 27.79772186279297\n",
      "Epoch 444 , Train Loss 27.79772186279297 , Test Loss 26.804140090942383\n",
      "Epoch 445 , Train Loss 26.804140090942383 , Test Loss 25.81297492980957\n",
      "Epoch 446 , Train Loss 25.81297492980957 , Test Loss 24.82496452331543\n",
      "Epoch 447 , Train Loss 24.82496452331543 , Test Loss 23.841026306152344\n",
      "Epoch 448 , Train Loss 23.841026306152344 , Test Loss 22.86216926574707\n",
      "Epoch 449 , Train Loss 22.86216926574707 , Test Loss 21.889671325683594\n",
      "Epoch 450 , Train Loss 21.889671325683594 , Test Loss 20.92475128173828\n",
      "Epoch 451 , Train Loss 20.92475128173828 , Test Loss 19.968944549560547\n",
      "Epoch 452 , Train Loss 19.968944549560547 , Test Loss 19.02361488342285\n",
      "Epoch 453 , Train Loss 19.02361488342285 , Test Loss 18.090167999267578\n",
      "Epoch 454 , Train Loss 18.090167999267578 , Test Loss 17.169862747192383\n",
      "Epoch 455 , Train Loss 17.169862747192383 , Test Loss 16.263811111450195\n",
      "Epoch 456 , Train Loss 16.263811111450195 , Test Loss 15.373002052307129\n",
      "Epoch 457 , Train Loss 15.373002052307129 , Test Loss 14.498083114624023\n",
      "Epoch 458 , Train Loss 14.498083114624023 , Test Loss 13.639497756958008\n",
      "Epoch 459 , Train Loss 13.639497756958008 , Test Loss 12.797297477722168\n",
      "Epoch 460 , Train Loss 12.797297477722168 , Test Loss 11.971351623535156\n",
      "Epoch 461 , Train Loss 11.971351623535156 , Test Loss 11.161494255065918\n",
      "Epoch 462 , Train Loss 11.161494255065918 , Test Loss 10.367201805114746\n",
      "Epoch 463 , Train Loss 10.367201805114746 , Test Loss 9.58782958984375\n",
      "Epoch 464 , Train Loss 9.58782958984375 , Test Loss 8.8226900100708\n",
      "Epoch 465 , Train Loss 8.8226900100708 , Test Loss 8.071012496948242\n",
      "Epoch 466 , Train Loss 8.071012496948242 , Test Loss 7.332014560699463\n",
      "Epoch 467 , Train Loss 7.332014560699463 , Test Loss 6.604907512664795\n",
      "Epoch 468 , Train Loss 6.604907512664795 , Test Loss 5.888837814331055\n",
      "Epoch 469 , Train Loss 5.888837814331055 , Test Loss 5.183034420013428\n",
      "Epoch 470 , Train Loss 5.183034420013428 , Test Loss 4.486750602722168\n",
      "Epoch 471 , Train Loss 4.486750602722168 , Test Loss 3.799365520477295\n",
      "Epoch 472 , Train Loss 3.799365520477295 , Test Loss 3.1202166080474854\n",
      "Epoch 473 , Train Loss 3.1202166080474854 , Test Loss 2.448820114135742\n",
      "Epoch 474 , Train Loss 2.448820114135742 , Test Loss 1.7847440242767334\n",
      "Epoch 475 , Train Loss 1.7847440242767334 , Test Loss 1.1278384923934937\n",
      "Epoch 476 , Train Loss 1.1278384923934937 , Test Loss 0.4786565899848938\n",
      "Epoch 477 , Train Loss 0.4786565899848938 , Test Loss 0.1676226556301117\n",
      "Epoch 478 , Train Loss 0.1676226556301117 , Test Loss 0.4995221495628357\n",
      "Epoch 479 , Train Loss 0.4995221495628357 , Test Loss 0.24372167885303497\n",
      "Epoch 480 , Train Loss 0.24372167885303497 , Test Loss 0.756158173084259\n",
      "Epoch 481 , Train Loss 0.756158173084259 , Test Loss 0.2438434511423111\n",
      "Epoch 482 , Train Loss 0.2438434511423111 , Test Loss 0.7560362815856934\n",
      "Epoch 483 , Train Loss 0.7560362815856934 , Test Loss 0.2439652532339096\n",
      "Epoch 484 , Train Loss 0.2439652532339096 , Test Loss 0.7559447288513184\n",
      "Epoch 485 , Train Loss 0.7559447288513184 , Test Loss 0.2440565973520279\n",
      "Epoch 486 , Train Loss 0.2440565973520279 , Test Loss 0.7558532357215881\n",
      "Epoch 487 , Train Loss 0.7558532357215881 , Test Loss 0.24414797127246857\n",
      "Epoch 488 , Train Loss 0.24414797127246857 , Test Loss 0.7557617425918579\n",
      "Epoch 489 , Train Loss 0.7557617425918579 , Test Loss 0.24423937499523163\n",
      "Epoch 490 , Train Loss 0.24423937499523163 , Test Loss 0.7557007670402527\n",
      "Epoch 491 , Train Loss 0.7557007670402527 , Test Loss 0.24430027604103088\n",
      "Epoch 492 , Train Loss 0.24430027604103088 , Test Loss 0.7556397914886475\n",
      "Epoch 493 , Train Loss 0.7556397914886475 , Test Loss 0.24436120688915253\n",
      "Epoch 494 , Train Loss 0.24436120688915253 , Test Loss 0.7555786371231079\n",
      "Epoch 495 , Train Loss 0.7555786371231079 , Test Loss 0.24442212283611298\n",
      "Epoch 496 , Train Loss 0.24442212283611298 , Test Loss 0.7555177211761475\n",
      "Epoch 497 , Train Loss 0.7555177211761475 , Test Loss 0.24448305368423462\n",
      "Epoch 498 , Train Loss 0.24448305368423462 , Test Loss 0.7554566860198975\n",
      "Epoch 499 , Train Loss 0.7554566860198975 , Test Loss 0.24454404413700104\n",
      "Epoch 500 , Train Loss 0.24454404413700104 , Test Loss 0.7553957104682922\n",
      "Epoch 501 , Train Loss 0.7553957104682922 , Test Loss 0.24460500478744507\n",
      "Epoch 502 , Train Loss 0.24460500478744507 , Test Loss 0.755334734916687\n",
      "Epoch 503 , Train Loss 0.755334734916687 , Test Loss 0.2446659505367279\n",
      "Epoch 504 , Train Loss 0.2446659505367279 , Test Loss 0.7553041577339172\n",
      "Epoch 505 , Train Loss 0.7553041577339172 , Test Loss 0.24469642341136932\n",
      "Epoch 506 , Train Loss 0.24469642341136932 , Test Loss 0.7552736401557922\n",
      "Epoch 507 , Train Loss 0.7552736401557922 , Test Loss 0.24472688138484955\n",
      "Epoch 508 , Train Loss 0.24472688138484955 , Test Loss 0.7552431225776672\n",
      "Epoch 509 , Train Loss 0.7552431225776672 , Test Loss 0.24475735425949097\n",
      "Epoch 510 , Train Loss 0.24475735425949097 , Test Loss 0.755212664604187\n",
      "Epoch 511 , Train Loss 0.755212664604187 , Test Loss 0.24478782713413239\n",
      "Epoch 512 , Train Loss 0.24478782713413239 , Test Loss 0.7551822066307068\n",
      "Epoch 513 , Train Loss 0.7551822066307068 , Test Loss 0.24481834471225739\n",
      "Epoch 514 , Train Loss 0.24481834471225739 , Test Loss 0.7551516890525818\n",
      "Epoch 515 , Train Loss 0.7551516890525818 , Test Loss 0.2448488026857376\n",
      "Epoch 516 , Train Loss 0.2448488026857376 , Test Loss 0.7551211714744568\n",
      "Epoch 517 , Train Loss 0.7551211714744568 , Test Loss 0.24487930536270142\n",
      "Epoch 518 , Train Loss 0.24487930536270142 , Test Loss 0.7550907135009766\n",
      "Epoch 519 , Train Loss 0.7550907135009766 , Test Loss 0.24490977823734283\n",
      "Epoch 520 , Train Loss 0.24490977823734283 , Test Loss 0.755060076713562\n",
      "Epoch 521 , Train Loss 0.755060076713562 , Test Loss 0.24494028091430664\n",
      "Epoch 522 , Train Loss 0.24494028091430664 , Test Loss 0.7550296187400818\n",
      "Epoch 523 , Train Loss 0.7550296187400818 , Test Loss 0.24497075378894806\n",
      "Epoch 524 , Train Loss 0.24497075378894806 , Test Loss 0.7549991607666016\n",
      "Epoch 525 , Train Loss 0.7549991607666016 , Test Loss 0.24500125646591187\n",
      "Epoch 526 , Train Loss 0.24500125646591187 , Test Loss 0.7549686431884766\n",
      "Epoch 527 , Train Loss 0.7549686431884766 , Test Loss 0.24503172934055328\n",
      "Epoch 528 , Train Loss 0.24503172934055328 , Test Loss 0.7549381256103516\n",
      "Epoch 529 , Train Loss 0.7549381256103516 , Test Loss 0.24506224691867828\n",
      "Epoch 530 , Train Loss 0.24506224691867828 , Test Loss 0.7549076676368713\n",
      "Epoch 531 , Train Loss 0.7549076676368713 , Test Loss 0.2450927495956421\n",
      "Epoch 532 , Train Loss 0.2450927495956421 , Test Loss 0.7548770308494568\n",
      "Epoch 533 , Train Loss 0.7548770308494568 , Test Loss 0.2451232522726059\n",
      "Epoch 534 , Train Loss 0.2451232522726059 , Test Loss 0.7548465728759766\n",
      "Epoch 535 , Train Loss 0.7548465728759766 , Test Loss 0.24515372514724731\n",
      "Epoch 536 , Train Loss 0.24515372514724731 , Test Loss 0.7548161745071411\n",
      "Epoch 537 , Train Loss 0.7548161745071411 , Test Loss 0.24518422782421112\n",
      "Epoch 538 , Train Loss 0.24518422782421112 , Test Loss 0.7547855973243713\n",
      "Epoch 539 , Train Loss 0.7547855973243713 , Test Loss 0.24521473050117493\n",
      "Epoch 540 , Train Loss 0.24521473050117493 , Test Loss 0.7547551989555359\n",
      "Epoch 541 , Train Loss 0.7547551989555359 , Test Loss 0.24524521827697754\n",
      "Epoch 542 , Train Loss 0.24524521827697754 , Test Loss 0.7547246217727661\n",
      "Epoch 543 , Train Loss 0.7547246217727661 , Test Loss 0.24527572095394135\n",
      "Epoch 544 , Train Loss 0.24527572095394135 , Test Loss 0.7546941041946411\n",
      "Epoch 545 , Train Loss 0.7546941041946411 , Test Loss 0.24530619382858276\n",
      "Epoch 546 , Train Loss 0.24530619382858276 , Test Loss 0.7546635270118713\n",
      "Epoch 547 , Train Loss 0.7546635270118713 , Test Loss 0.24533672630786896\n",
      "Epoch 548 , Train Loss 0.24533672630786896 , Test Loss 0.7546330690383911\n",
      "Epoch 549 , Train Loss 0.7546330690383911 , Test Loss 0.24536724388599396\n",
      "Epoch 550 , Train Loss 0.24536724388599396 , Test Loss 0.7546024918556213\n",
      "Epoch 551 , Train Loss 0.7546024918556213 , Test Loss 0.24539774656295776\n",
      "Epoch 552 , Train Loss 0.24539774656295776 , Test Loss 0.7545719742774963\n",
      "Epoch 553 , Train Loss 0.7545719742774963 , Test Loss 0.24542824923992157\n",
      "Epoch 554 , Train Loss 0.24542824923992157 , Test Loss 0.7545415759086609\n",
      "Epoch 555 , Train Loss 0.7545415759086609 , Test Loss 0.24545875191688538\n",
      "Epoch 556 , Train Loss 0.24545875191688538 , Test Loss 0.7545109987258911\n",
      "Epoch 557 , Train Loss 0.7545109987258911 , Test Loss 0.24548925459384918\n",
      "Epoch 558 , Train Loss 0.24548925459384918 , Test Loss 0.7544804811477661\n",
      "Epoch 559 , Train Loss 0.7544804811477661 , Test Loss 0.245519757270813\n",
      "Epoch 560 , Train Loss 0.245519757270813 , Test Loss 0.7544804811477661\n",
      "Epoch 561 , Train Loss 0.7544804811477661 , Test Loss 0.245519757270813\n",
      "Epoch 562 , Train Loss 0.245519757270813 , Test Loss 0.7544804811477661\n",
      "Epoch 563 , Train Loss 0.7544804811477661 , Test Loss 0.2455197274684906\n",
      "Epoch 564 , Train Loss 0.2455197274684906 , Test Loss 0.7544805407524109\n",
      "Epoch 565 , Train Loss 0.7544805407524109 , Test Loss 0.245519757270813\n",
      "Epoch 566 , Train Loss 0.245519757270813 , Test Loss 0.7544806003570557\n",
      "Epoch 567 , Train Loss 0.7544806003570557 , Test Loss 0.2455196976661682\n",
      "Epoch 568 , Train Loss 0.2455196976661682 , Test Loss 0.7544805407524109\n",
      "Epoch 569 , Train Loss 0.7544805407524109 , Test Loss 0.2455197274684906\n",
      "Epoch 570 , Train Loss 0.2455197274684906 , Test Loss 0.7544804811477661\n",
      "Epoch 571 , Train Loss 0.7544804811477661 , Test Loss 0.24551966786384583\n",
      "Epoch 572 , Train Loss 0.24551966786384583 , Test Loss 0.7544804811477661\n",
      "Epoch 573 , Train Loss 0.7544804811477661 , Test Loss 0.2455196976661682\n",
      "Epoch 574 , Train Loss 0.2455196976661682 , Test Loss 0.7544804811477661\n",
      "Epoch 575 , Train Loss 0.7544804811477661 , Test Loss 0.24551966786384583\n",
      "Epoch 576 , Train Loss 0.24551966786384583 , Test Loss 0.7544804811477661\n",
      "Epoch 577 , Train Loss 0.7544804811477661 , Test Loss 0.24551966786384583\n",
      "Epoch 578 , Train Loss 0.24551966786384583 , Test Loss 0.7544804811477661\n",
      "Epoch 579 , Train Loss 0.7544804811477661 , Test Loss 0.24551965296268463\n",
      "Epoch 580 , Train Loss 0.24551965296268463 , Test Loss 0.7544804811477661\n",
      "Epoch 581 , Train Loss 0.7544804811477661 , Test Loss 0.24551965296268463\n",
      "Epoch 582 , Train Loss 0.24551965296268463 , Test Loss 0.7544805407524109\n",
      "Epoch 583 , Train Loss 0.7544805407524109 , Test Loss 0.24551962316036224\n",
      "Epoch 584 , Train Loss 0.24551962316036224 , Test Loss 0.7544805407524109\n",
      "Epoch 585 , Train Loss 0.7544805407524109 , Test Loss 0.24551962316036224\n",
      "Epoch 586 , Train Loss 0.24551962316036224 , Test Loss 0.7544805407524109\n",
      "Epoch 587 , Train Loss 0.7544805407524109 , Test Loss 0.24551962316036224\n",
      "Epoch 588 , Train Loss 0.24551962316036224 , Test Loss 0.7544806003570557\n",
      "Epoch 589 , Train Loss 0.7544806003570557 , Test Loss 0.24551962316036224\n",
      "Epoch 590 , Train Loss 0.24551962316036224 , Test Loss 0.7544805407524109\n",
      "Epoch 591 , Train Loss 0.7544805407524109 , Test Loss 0.24551959335803986\n",
      "Epoch 592 , Train Loss 0.24551959335803986 , Test Loss 0.7544806003570557\n",
      "Epoch 593 , Train Loss 0.7544806003570557 , Test Loss 0.24551959335803986\n",
      "Epoch 594 , Train Loss 0.24551959335803986 , Test Loss 0.7544806599617004\n",
      "Epoch 595 , Train Loss 0.7544806599617004 , Test Loss 0.24551957845687866\n",
      "Epoch 596 , Train Loss 0.24551957845687866 , Test Loss 0.7544806003570557\n",
      "Epoch 597 , Train Loss 0.7544806003570557 , Test Loss 0.24551962316036224\n",
      "Epoch 598 , Train Loss 0.24551962316036224 , Test Loss 0.7544806599617004\n",
      "Epoch 599 , Train Loss 0.7544806599617004 , Test Loss 0.24551959335803986\n",
      "Epoch 600 , Train Loss 0.24551959335803986 , Test Loss 0.7544806003570557\n",
      "Epoch 601 , Train Loss 0.7544806003570557 , Test Loss 0.24551959335803986\n",
      "Epoch 602 , Train Loss 0.24551959335803986 , Test Loss 0.7544806599617004\n",
      "Epoch 603 , Train Loss 0.7544806599617004 , Test Loss 0.24551959335803986\n",
      "Epoch 604 , Train Loss 0.24551959335803986 , Test Loss 0.7544806003570557\n",
      "Epoch 605 , Train Loss 0.7544806003570557 , Test Loss 0.24551954865455627\n",
      "Epoch 606 , Train Loss 0.24551954865455627 , Test Loss 0.7544806599617004\n",
      "Epoch 607 , Train Loss 0.7544806599617004 , Test Loss 0.24551957845687866\n",
      "Epoch 608 , Train Loss 0.24551957845687866 , Test Loss 0.7544806003570557\n",
      "Epoch 609 , Train Loss 0.7544806003570557 , Test Loss 0.24551954865455627\n",
      "Epoch 610 , Train Loss 0.24551954865455627 , Test Loss 0.7544806003570557\n",
      "Epoch 611 , Train Loss 0.7544806003570557 , Test Loss 0.24551954865455627\n",
      "Epoch 612 , Train Loss 0.24551954865455627 , Test Loss 0.7544806003570557\n",
      "Epoch 613 , Train Loss 0.7544806003570557 , Test Loss 0.24551954865455627\n",
      "Epoch 614 , Train Loss 0.24551954865455627 , Test Loss 0.7544806599617004\n",
      "Epoch 615 , Train Loss 0.7544806599617004 , Test Loss 0.2455195188522339\n",
      "Epoch 616 , Train Loss 0.2455195188522339 , Test Loss 0.7544807195663452\n",
      "Epoch 617 , Train Loss 0.7544807195663452 , Test Loss 0.2455195188522339\n",
      "Epoch 618 , Train Loss 0.2455195188522339 , Test Loss 0.7544807195663452\n",
      "Epoch 619 , Train Loss 0.7544807195663452 , Test Loss 0.2455195188522339\n",
      "Epoch 620 , Train Loss 0.2455195188522339 , Test Loss 0.7544806599617004\n",
      "Epoch 621 , Train Loss 0.7544806599617004 , Test Loss 0.2455195188522339\n",
      "Epoch 622 , Train Loss 0.2455195188522339 , Test Loss 0.7544807195663452\n",
      "Epoch 623 , Train Loss 0.7544807195663452 , Test Loss 0.2455195039510727\n",
      "Epoch 624 , Train Loss 0.2455195039510727 , Test Loss 0.7544807195663452\n",
      "Epoch 625 , Train Loss 0.7544807195663452 , Test Loss 0.2455195039510727\n",
      "Epoch 626 , Train Loss 0.2455195039510727 , Test Loss 0.7544807195663452\n",
      "Epoch 627 , Train Loss 0.7544807195663452 , Test Loss 0.2455194741487503\n",
      "Epoch 628 , Train Loss 0.2455194741487503 , Test Loss 0.7544807195663452\n",
      "Epoch 629 , Train Loss 0.7544807195663452 , Test Loss 0.2455194741487503\n",
      "Epoch 630 , Train Loss 0.2455194741487503 , Test Loss 0.7544806599617004\n",
      "Epoch 631 , Train Loss 0.7544806599617004 , Test Loss 0.2455194741487503\n",
      "Epoch 632 , Train Loss 0.2455194741487503 , Test Loss 0.7544806003570557\n",
      "Epoch 633 , Train Loss 0.7544806003570557 , Test Loss 0.2455195039510727\n",
      "Epoch 634 , Train Loss 0.2455195039510727 , Test Loss 0.7544807195663452\n",
      "Epoch 635 , Train Loss 0.7544807195663452 , Test Loss 0.2455194741487503\n",
      "Epoch 636 , Train Loss 0.2455194741487503 , Test Loss 0.7544807195663452\n",
      "Epoch 637 , Train Loss 0.7544807195663452 , Test Loss 0.24551944434642792\n",
      "Epoch 638 , Train Loss 0.24551944434642792 , Test Loss 0.7544807195663452\n",
      "Epoch 639 , Train Loss 0.7544807195663452 , Test Loss 0.2455194741487503\n",
      "Epoch 640 , Train Loss 0.2455194741487503 , Test Loss 0.7544807195663452\n",
      "Epoch 641 , Train Loss 0.7544807195663452 , Test Loss 0.24551944434642792\n",
      "Epoch 642 , Train Loss 0.24551944434642792 , Test Loss 0.75448077917099\n",
      "Epoch 643 , Train Loss 0.75448077917099 , Test Loss 0.24551944434642792\n",
      "Epoch 644 , Train Loss 0.24551944434642792 , Test Loss 0.75448077917099\n",
      "Epoch 645 , Train Loss 0.75448077917099 , Test Loss 0.24551942944526672\n",
      "Epoch 646 , Train Loss 0.24551942944526672 , Test Loss 0.75448077917099\n",
      "Epoch 647 , Train Loss 0.75448077917099 , Test Loss 0.24551944434642792\n",
      "Epoch 648 , Train Loss 0.24551944434642792 , Test Loss 0.75448077917099\n",
      "Epoch 649 , Train Loss 0.75448077917099 , Test Loss 0.24551944434642792\n",
      "Epoch 650 , Train Loss 0.24551944434642792 , Test Loss 0.75448077917099\n",
      "Epoch 651 , Train Loss 0.75448077917099 , Test Loss 0.24551944434642792\n",
      "Epoch 652 , Train Loss 0.24551944434642792 , Test Loss 0.7544807195663452\n",
      "Epoch 653 , Train Loss 0.7544807195663452 , Test Loss 0.24551939964294434\n",
      "Epoch 654 , Train Loss 0.24551939964294434 , Test Loss 0.7544806599617004\n",
      "Epoch 655 , Train Loss 0.7544806599617004 , Test Loss 0.24551942944526672\n",
      "Epoch 656 , Train Loss 0.24551942944526672 , Test Loss 0.7544807195663452\n",
      "Epoch 657 , Train Loss 0.7544807195663452 , Test Loss 0.24551939964294434\n",
      "Epoch 658 , Train Loss 0.24551939964294434 , Test Loss 0.75448077917099\n",
      "Epoch 659 , Train Loss 0.75448077917099 , Test Loss 0.24551942944526672\n",
      "Epoch 660 , Train Loss 0.24551942944526672 , Test Loss 0.75448077917099\n",
      "Epoch 661 , Train Loss 0.75448077917099 , Test Loss 0.24551942944526672\n",
      "Epoch 662 , Train Loss 0.24551942944526672 , Test Loss 0.75448077917099\n",
      "Epoch 663 , Train Loss 0.75448077917099 , Test Loss 0.24551939964294434\n",
      "Epoch 664 , Train Loss 0.24551939964294434 , Test Loss 0.75448077917099\n",
      "Epoch 665 , Train Loss 0.75448077917099 , Test Loss 0.24551939964294434\n",
      "Epoch 666 , Train Loss 0.24551939964294434 , Test Loss 0.75448077917099\n",
      "Epoch 667 , Train Loss 0.75448077917099 , Test Loss 0.24551942944526672\n",
      "Epoch 668 , Train Loss 0.24551942944526672 , Test Loss 0.75448077917099\n",
      "Epoch 669 , Train Loss 0.75448077917099 , Test Loss 0.24551942944526672\n",
      "Epoch 670 , Train Loss 0.24551942944526672 , Test Loss 0.75448077917099\n",
      "Epoch 671 , Train Loss 0.75448077917099 , Test Loss 0.24551939964294434\n",
      "Epoch 672 , Train Loss 0.24551939964294434 , Test Loss 0.7544806599617004\n",
      "Epoch 673 , Train Loss 0.7544806599617004 , Test Loss 0.24551942944526672\n",
      "Epoch 674 , Train Loss 0.24551942944526672 , Test Loss 0.75448077917099\n",
      "Epoch 675 , Train Loss 0.75448077917099 , Test Loss 0.24551939964294434\n",
      "Epoch 676 , Train Loss 0.24551939964294434 , Test Loss 0.75448077917099\n",
      "Epoch 677 , Train Loss 0.75448077917099 , Test Loss 0.24551936984062195\n",
      "Epoch 678 , Train Loss 0.24551936984062195 , Test Loss 0.75448077917099\n",
      "Epoch 679 , Train Loss 0.75448077917099 , Test Loss 0.24551939964294434\n",
      "Epoch 680 , Train Loss 0.24551939964294434 , Test Loss 0.75448077917099\n",
      "Epoch 681 , Train Loss 0.75448077917099 , Test Loss 0.24551935493946075\n",
      "Epoch 682 , Train Loss 0.24551935493946075 , Test Loss 0.7544807195663452\n",
      "Epoch 683 , Train Loss 0.7544807195663452 , Test Loss 0.24551936984062195\n",
      "Epoch 684 , Train Loss 0.24551936984062195 , Test Loss 0.75448077917099\n",
      "Epoch 685 , Train Loss 0.75448077917099 , Test Loss 0.24551936984062195\n",
      "Epoch 686 , Train Loss 0.24551936984062195 , Test Loss 0.7544807195663452\n",
      "Epoch 687 , Train Loss 0.7544807195663452 , Test Loss 0.24551936984062195\n",
      "Epoch 688 , Train Loss 0.24551936984062195 , Test Loss 0.7544807195663452\n",
      "Epoch 689 , Train Loss 0.7544807195663452 , Test Loss 0.24551936984062195\n",
      "Epoch 690 , Train Loss 0.24551936984062195 , Test Loss 0.7544807195663452\n",
      "Epoch 691 , Train Loss 0.7544807195663452 , Test Loss 0.24551935493946075\n",
      "Epoch 692 , Train Loss 0.24551935493946075 , Test Loss 0.75448077917099\n",
      "Epoch 693 , Train Loss 0.75448077917099 , Test Loss 0.24551936984062195\n",
      "Epoch 694 , Train Loss 0.24551936984062195 , Test Loss 0.7544807195663452\n",
      "Epoch 695 , Train Loss 0.7544807195663452 , Test Loss 0.24551932513713837\n",
      "Epoch 696 , Train Loss 0.24551932513713837 , Test Loss 0.75448077917099\n",
      "Epoch 697 , Train Loss 0.75448077917099 , Test Loss 0.24551935493946075\n",
      "Epoch 698 , Train Loss 0.24551935493946075 , Test Loss 0.75448077917099\n",
      "Epoch 699 , Train Loss 0.75448077917099 , Test Loss 0.24551935493946075\n",
      "Epoch 700 , Train Loss 0.24551935493946075 , Test Loss 0.75448077917099\n",
      "Epoch 701 , Train Loss 0.75448077917099 , Test Loss 0.24551935493946075\n",
      "Epoch 702 , Train Loss 0.24551935493946075 , Test Loss 0.75448077917099\n",
      "Epoch 703 , Train Loss 0.75448077917099 , Test Loss 0.24551935493946075\n",
      "Epoch 704 , Train Loss 0.24551935493946075 , Test Loss 0.75448077917099\n",
      "Epoch 705 , Train Loss 0.75448077917099 , Test Loss 0.24551932513713837\n",
      "Epoch 706 , Train Loss 0.24551932513713837 , Test Loss 0.7544808387756348\n",
      "Epoch 707 , Train Loss 0.7544808387756348 , Test Loss 0.24551932513713837\n",
      "Epoch 708 , Train Loss 0.24551932513713837 , Test Loss 0.7544808387756348\n",
      "Epoch 709 , Train Loss 0.7544808387756348 , Test Loss 0.24551932513713837\n",
      "Epoch 710 , Train Loss 0.24551932513713837 , Test Loss 0.75448077917099\n",
      "Epoch 711 , Train Loss 0.75448077917099 , Test Loss 0.24551932513713837\n",
      "Epoch 712 , Train Loss 0.24551932513713837 , Test Loss 0.75448077917099\n",
      "Epoch 713 , Train Loss 0.75448077917099 , Test Loss 0.24551932513713837\n",
      "Epoch 714 , Train Loss 0.24551932513713837 , Test Loss 0.7544808387756348\n",
      "Epoch 715 , Train Loss 0.7544808387756348 , Test Loss 0.24551932513713837\n",
      "Epoch 716 , Train Loss 0.24551932513713837 , Test Loss 0.7544808387756348\n",
      "Epoch 717 , Train Loss 0.7544808387756348 , Test Loss 0.24551928043365479\n",
      "Epoch 718 , Train Loss 0.24551928043365479 , Test Loss 0.7544808387756348\n",
      "Epoch 719 , Train Loss 0.7544808387756348 , Test Loss 0.24551928043365479\n",
      "Epoch 720 , Train Loss 0.24551928043365479 , Test Loss 0.75448077917099\n",
      "Epoch 721 , Train Loss 0.75448077917099 , Test Loss 0.24551932513713837\n",
      "Epoch 722 , Train Loss 0.24551932513713837 , Test Loss 0.7544808387756348\n",
      "Epoch 723 , Train Loss 0.7544808387756348 , Test Loss 0.24551929533481598\n",
      "Epoch 724 , Train Loss 0.24551929533481598 , Test Loss 0.75448077917099\n",
      "Epoch 725 , Train Loss 0.75448077917099 , Test Loss 0.24551932513713837\n",
      "Epoch 726 , Train Loss 0.24551932513713837 , Test Loss 0.7544808387756348\n",
      "Epoch 727 , Train Loss 0.7544808387756348 , Test Loss 0.24551929533481598\n",
      "Epoch 728 , Train Loss 0.24551929533481598 , Test Loss 0.7544808387756348\n",
      "Epoch 729 , Train Loss 0.7544808387756348 , Test Loss 0.24551929533481598\n",
      "Epoch 730 , Train Loss 0.24551929533481598 , Test Loss 0.7544808387756348\n",
      "Epoch 731 , Train Loss 0.7544808387756348 , Test Loss 0.24551929533481598\n",
      "Epoch 732 , Train Loss 0.24551929533481598 , Test Loss 0.7544808387756348\n",
      "Epoch 733 , Train Loss 0.7544808387756348 , Test Loss 0.24551929533481598\n",
      "Epoch 734 , Train Loss 0.24551929533481598 , Test Loss 0.7544808387756348\n",
      "Epoch 735 , Train Loss 0.7544808387756348 , Test Loss 0.24551932513713837\n",
      "Epoch 736 , Train Loss 0.24551932513713837 , Test Loss 0.7544808387756348\n",
      "Epoch 737 , Train Loss 0.7544808387756348 , Test Loss 0.24551928043365479\n",
      "Epoch 738 , Train Loss 0.24551928043365479 , Test Loss 0.7544808387756348\n",
      "Epoch 739 , Train Loss 0.7544808387756348 , Test Loss 0.24551929533481598\n",
      "Epoch 740 , Train Loss 0.24551929533481598 , Test Loss 0.7544808387756348\n",
      "Epoch 741 , Train Loss 0.7544808387756348 , Test Loss 0.24551929533481598\n",
      "Epoch 742 , Train Loss 0.24551929533481598 , Test Loss 0.7544808387756348\n",
      "Epoch 743 , Train Loss 0.7544808387756348 , Test Loss 0.24551928043365479\n",
      "Epoch 744 , Train Loss 0.24551928043365479 , Test Loss 0.7544808387756348\n",
      "Epoch 745 , Train Loss 0.7544808387756348 , Test Loss 0.24551928043365479\n",
      "Epoch 746 , Train Loss 0.24551928043365479 , Test Loss 0.7544808387756348\n",
      "Epoch 747 , Train Loss 0.7544808387756348 , Test Loss 0.24551929533481598\n",
      "Epoch 748 , Train Loss 0.24551929533481598 , Test Loss 0.7544808387756348\n",
      "Epoch 749 , Train Loss 0.7544808387756348 , Test Loss 0.24551928043365479\n",
      "Epoch 750 , Train Loss 0.24551928043365479 , Test Loss 0.7544808387756348\n",
      "Epoch 751 , Train Loss 0.7544808387756348 , Test Loss 0.2455192506313324\n",
      "Epoch 752 , Train Loss 0.2455192506313324 , Test Loss 0.7544808387756348\n",
      "Epoch 753 , Train Loss 0.7544808387756348 , Test Loss 0.24551928043365479\n",
      "Epoch 754 , Train Loss 0.24551928043365479 , Test Loss 0.7544808387756348\n",
      "Epoch 755 , Train Loss 0.7544808387756348 , Test Loss 0.24551928043365479\n",
      "Epoch 756 , Train Loss 0.24551928043365479 , Test Loss 0.7544808387756348\n",
      "Epoch 757 , Train Loss 0.7544808387756348 , Test Loss 0.24551928043365479\n",
      "Epoch 758 , Train Loss 0.24551928043365479 , Test Loss 0.7544808387756348\n",
      "Epoch 759 , Train Loss 0.7544808387756348 , Test Loss 0.2455192506313324\n",
      "Epoch 760 , Train Loss 0.2455192506313324 , Test Loss 0.7544808387756348\n",
      "Epoch 761 , Train Loss 0.7544808387756348 , Test Loss 0.2455192506313324\n",
      "Epoch 762 , Train Loss 0.2455192506313324 , Test Loss 0.7544808387756348\n",
      "Epoch 763 , Train Loss 0.7544808387756348 , Test Loss 0.2455192506313324\n",
      "Epoch 764 , Train Loss 0.2455192506313324 , Test Loss 0.7544808983802795\n",
      "Epoch 765 , Train Loss 0.7544808983802795 , Test Loss 0.2455192506313324\n",
      "Epoch 766 , Train Loss 0.2455192506313324 , Test Loss 0.7544808387756348\n",
      "Epoch 767 , Train Loss 0.7544808387756348 , Test Loss 0.2455192506313324\n",
      "Epoch 768 , Train Loss 0.2455192506313324 , Test Loss 0.7544808387756348\n",
      "Epoch 769 , Train Loss 0.7544808387756348 , Test Loss 0.2455192506313324\n",
      "Epoch 770 , Train Loss 0.2455192506313324 , Test Loss 0.7544808387756348\n",
      "Epoch 771 , Train Loss 0.7544808387756348 , Test Loss 0.2455192506313324\n",
      "Epoch 772 , Train Loss 0.2455192506313324 , Test Loss 0.7544808387756348\n",
      "Epoch 773 , Train Loss 0.7544808387756348 , Test Loss 0.2455192506313324\n",
      "Epoch 774 , Train Loss 0.2455192506313324 , Test Loss 0.7544808983802795\n",
      "Epoch 775 , Train Loss 0.7544808983802795 , Test Loss 0.2455192506313324\n",
      "Epoch 776 , Train Loss 0.2455192506313324 , Test Loss 0.7544808983802795\n",
      "Epoch 777 , Train Loss 0.7544808983802795 , Test Loss 0.2455192506313324\n",
      "Epoch 778 , Train Loss 0.2455192506313324 , Test Loss 0.75448077917099\n",
      "Epoch 779 , Train Loss 0.75448077917099 , Test Loss 0.24551922082901\n",
      "Epoch 780 , Train Loss 0.24551922082901 , Test Loss 0.7544808983802795\n",
      "Epoch 781 , Train Loss 0.7544808983802795 , Test Loss 0.24551922082901\n",
      "Epoch 782 , Train Loss 0.24551922082901 , Test Loss 0.7544808983802795\n",
      "Epoch 783 , Train Loss 0.7544808983802795 , Test Loss 0.24551922082901\n",
      "Epoch 784 , Train Loss 0.24551922082901 , Test Loss 0.7544808983802795\n",
      "Epoch 785 , Train Loss 0.7544808983802795 , Test Loss 0.24551922082901\n",
      "Epoch 786 , Train Loss 0.24551922082901 , Test Loss 0.7544808983802795\n",
      "Epoch 787 , Train Loss 0.7544808983802795 , Test Loss 0.2455192506313324\n",
      "Epoch 788 , Train Loss 0.2455192506313324 , Test Loss 0.7544808983802795\n",
      "Epoch 789 , Train Loss 0.7544808983802795 , Test Loss 0.2455192506313324\n",
      "Epoch 790 , Train Loss 0.2455192506313324 , Test Loss 0.75448077917099\n",
      "Epoch 791 , Train Loss 0.75448077917099 , Test Loss 0.24551922082901\n",
      "Epoch 792 , Train Loss 0.24551922082901 , Test Loss 0.7544808983802795\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, X, Y, Z, loss_function, optimizer, epoches)\u001b[0m\n\u001b[0;32m      9\u001b[0m predict \u001b[38;5;241m=\u001b[39m model(X,Y)\n\u001b[0;32m     10\u001b[0m tr_loss \u001b[38;5;241m=\u001b[39m loss_function(predict , Z)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mtr_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32md:\\GitHub_Repo\\torchstudy\\.venv\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\GitHub_Repo\\torchstudy\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\GitHub_Repo\\torchstudy\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model2 , X , Y , Z , loss_fn , optimizer , 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w1': 5, 'w2': 7, 'b': 499}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = round(model2.weight1.cpu().detach().tolist()[0])\n",
    "w2 = round(model2.weight2.cpu().detach().tolist()[0])\n",
    "b = round(model2.bias.cpu().detach().tolist()[0])\n",
    "\n",
    "weights = {}\n",
    "weights['w1'] = w1\n",
    "weights['w2'] = w2\n",
    "weights['b'] = b\n",
    "\n",
    "weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model2.state_dict() , 'model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
